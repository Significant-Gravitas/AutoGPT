# Copilot Issues - Root Cause Analysis

## Issue #1: Stream timeout toast appearing on every chat

**User Report**: Toast saying "Stream timed out. The server took too long to respond."

**Frontend Code** (`useCopilotPage.ts:174-187`):
```typescript
useEffect(() => {
  if (status !== "submitted") return;

  const timer = setTimeout(() => {
    stopRef.current();
    toast({
      title: "Stream timed out",
      description: "The server took too long to respond. Please try again.",
      variant: "destructive",
    });
  }, STREAM_START_TIMEOUT_MS); // 12000ms = 12 seconds

  return () => clearTimeout(timer);
}, [status]);
```

**Root Cause**:
- Frontend status transitions: `ready` → `submitted` (when sending) → `streaming` (when first chunk arrives)
- If status stays `submitted` for >12s, timeout fires
- Status only changes to `streaming` when the FIRST SSE chunk arrives
- If backend doesn't send first chunk within 12s, toast appears

**What Fixed**:
- ✅ `block=None` fix helps xread return immediately

**What Might Still Be Broken**:
- ❓ Does copilot executor START quickly enough after enqueue?
- ❓ Is the first chunk published to Redis immediately?
- ❓ Does _stream_listener deliver chunks to the queue promptly?

**Investigation Needed**:
1. Check executor startup time from RabbitMQ enqueue to first chunk
2. Check if _stream_listener is staying alive (not being cancelled like before)
3. Verify chunks flow: executor → publish_chunk → Redis → subscriber_queue → SSE

---

## Issue #2: Chat not loading response unless retried

**User Report**: "reply only shown after I refreshed"

**Possible Causes**:
- Frontend not receiving SSE events
- Frontend receiving events but not updating UI state
- Backend sending chunks but SSE connection failing
- Chunks being published but subscriber_queue not receiving them

**Investigation Needed**:
1. Check browser console for SSE connection errors
2. Check if chunks are being published to Redis
3. Check if subscriber_queue is receiving chunks
4. Check if React state is updating when chunks arrive

---

## Issue #3: Updates happening in batch instead of real-time streaming

**User Report**: "the update is happening at batch instead"

**Frontend Code** (AI SDK `useChat`):
- Uses Server-Sent Events (SSE) to receive chunks
- Should update UI on each chunk

**Backend Flow**:
1. Executor generates chunks from OpenAI stream
2. Chunks published to Redis via `publish_chunk()`
3. `_stream_listener` reads from Redis with `block=None`
4. Chunks put into subscriber_queue
5. SSE event_generator reads from queue and yields

**Possible Causes**:
- ❓ Executor buffering chunks before publishing?
- ❓ React batching state updates?
- ❓ SSE buffering on network/proxy layer?
- ❓ _stream_listener polling too slowly?

**Investigation Needed**:
1. Check if executor is yielding chunks immediately or buffering
2. Check _stream_listener polling frequency
3. Check if chunks are being published with timestamps to measure delay

---

## Issue #4: Session stuck on red button (loading state)

**User Report**: "red button" or "greyed out button"

**Frontend Code** (`useCopilotPage.ts`):
- Loading state controlled by `status` from `useChat`
- Status should be: `ready` (idle) → `submitted` → `streaming` → `ready` (done)
- Button disabled while `status !== "ready"`

**Root Cause**:
- Status never returns to `ready` because:
  - StreamFinish never arrives, OR
  - SSE stream doesn't properly close with `data: [DONE]\n\n`

**Backend Code** (`routes.py:560-573`):
```python
if isinstance(chunk, StreamFinish):
    total_time = time_module.perf_counter() - event_gen_start
    logger.info(f"[TIMING] StreamFinish received in {total_time:.2f}s")
    break  # Exit the loop
```

**Backend Code** (`routes.py:628`):
```python
finally:
    yield "data: [DONE]\n\n"  # Always sent
```

**Investigation Needed**:
1. Is StreamFinish being generated by the executor?
2. Is StreamFinish being published to Redis?
3. Is StreamFinish reaching the subscriber_queue?
4. Is `data: [DONE]\n\n` being sent to frontend?

---

## Issue #5: Agent execution dropping after first tool call

**User Report**: "I asked to make an agent, it run a single tool and die"

**Possible Causes**:
- Tool execution completes but doesn't continue to next step
- Long-running tools timeout or error
- Executor doesn't resume after tool completion
- Missing continuation logic

**Investigation Needed**:
1. Check tool execution flow in executor
2. Check if tools are marked as "long-running" correctly
3. Check completion handlers for synchronous vs async tools
4. Review agent generation tool specifically

---

## Issue #6: Second chat showing introduction again

**User Report**: "I ask my name again, reply only shown after I refreshed" + repeated Otto introduction

**Frontend Code** (`useCopilotPage.ts:191-198`):
```typescript
useEffect(() => {
  if (!hydratedMessages || hydratedMessages.length === 0) return;
  if (status === "streaming" || status === "submitted") return;
  setMessages((prev) => {
    if (prev.length >= hydratedMessages.length) return prev;
    return deduplicateMessages(hydratedMessages);
  });
}, [hydratedMessages, setMessages, status]);
```

**Possible Causes**:
- Session history not being saved correctly
- Frontend not sending conversation context to backend
- Backend not including previous messages in prompt
- Each request treated as new conversation

**Backend Code** (`routes.py:437-450`):
```python
if request.message:
    message = ChatMessage(
        role="user" if request.is_user_message else "assistant",
        content=request.message,
    )
    await append_and_save_message(session_id, message)
```

**Backend Code** (`service.py:438-451`):
```python
if message and (
    len(session.messages) == 0
    or not (
        session.messages[-1].role == new_message_role
        and session.messages[-1].content == message
    )
):
    session.messages.append(ChatMessage(role=new_message_role, content=message))
```

**Investigation Needed**:
1. Check if messages are being saved to session correctly
2. Check if session.messages includes full history when generating response
3. Check if OpenAI API call includes previous messages
4. Check if introduction logic is repeated

---

## Summary of Fixes Needed

### Backend Issues:
1. ✅ **FIXED**: `block=None` in stream_registry.py (xread hanging)
2. ❓ **INVESTIGATE**: Executor startup delay
3. ❓ **INVESTIGATE**: StreamFinish generation and delivery
4. ❓ **INVESTIGATE**: Tool execution continuation
5. ❓ **INVESTIGATE**: Message history handling

### Frontend Issues:
1. ❓ **INVESTIGATE**: SSE connection reliability
2. ❓ **INVESTIGATE**: React state update batching
3. ❓ **INVESTIGATE**: Status transition handling

### Both:
1. ❓ **INVESTIGATE**: End-to-end timing from request → first chunk
2. ❓ **INVESTIGATE**: Chunk delivery pipeline integrity
