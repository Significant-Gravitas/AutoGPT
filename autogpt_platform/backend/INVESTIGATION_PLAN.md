# Systematic Investigation Plan

## What I Fixed

✅ **Line 375** in `subscribe_to_task`: Changed `block=0` → `block=None`
- This fixes the REPLAY phase (getting old messages from Redis)
- Prevents hanging when replaying missed messages
- **Does NOT** fix the live streaming phase

## What I Did NOT Fix

❌ **_stream_listener** (line 497): Still uses `block=30000` (intentional)
- This is CORRECT - it should block up to 30s waiting for new messages
- The problem isn't the blocking behavior here
- The problem was that _stream_listener was being CANCELLED prematurely

## Root Cause Analysis Needed

### Issue #1: Timeout Toast

**Question**: Why does the first chunk take > 12 seconds?
**Investigation Steps**:
1. Add timing logs: `enqueue_task` → `executor starts` → `first chunk published`
2. Check if executor is starting quickly from RabbitMQ
3. Check if copilot service is yielding first chunk quickly
4. Check if first chunk is being published to Redis immediately

**Files to Check**:
- `backend/copilot/executor/processor.py` - executor startup
- `backend/copilot/service.py` - chunk generation
- Backend logs for timing

### Issue #2: Response Not Loading

**Question**: Why doesn't the response appear without refresh?
**Investigation Steps**:
1. Check if chunks are reaching Redis
2. Check if subscriber_queue is receiving chunks
3. Check if SSE is delivering chunks to frontend
4. Check browser console for errors

**Frontend Files**:
- `useCopilotPage.ts` - SSE handling
- Browser Network tab - check `/stream` endpoint

### Issue #3: Batched Updates

**Question**: Why do chunks appear all at once instead of streaming?
**Investigation Steps**:
1. Check if executor is buffering chunks before publishing
2. Check how frequently _stream_listener polls (it should poll every 30s OR when messages arrive)
3. Check if React is batching state updates
4. Check if there's network buffering (nginx, vercel, etc.)

**Key Insight**: With `block=30000`, _stream_listener waits UP TO 30s. But it returns IMMEDIATELY when messages arrive. So this should be fine for real-time streaming.

### Issue #4: Red Button Stuck

**Question**: Why doesn't the loading state clear?
**Investigation Steps**:
1. Check if StreamFinish is being generated by executor
2. Check if StreamFinish is being published to Redis
3. Check if StreamFinish reaches subscriber_queue
4. Check if `data: [DONE]\n\n` is sent to frontend
5. Check if frontend status transitions properly

**Backend**: Check `mark_task_completed()` - does it publish StreamFinish?
**Frontend**: Check `useChat` status transitions

### Issue #5: Agent Tools Dropping

**Question**: Why does execution stop after first tool?
**Investigation Steps**:
1. Check tool execution completion handling
2. Check if continuation is triggered after tool completes
3. Check if synchronous vs async tools are handled differently
4. Review agent generation tool specifically

**Files**:
- `backend/copilot/tools/create_agent.py`
- `backend/copilot/tools/edit_agent.py`
- `backend/copilot/completion_handler.py`

### Issue #6: Repeated Introduction

**Question**: Why is conversation context lost?
**Investigation Steps**:
1. Check if messages are being saved to session correctly
2. Check if session.messages includes full history when generating response
3. Check if system prompt includes previous context
4. Check if frontend is sending previous messages

**Backend**:
- `append_and_save_message()` - saves messages
- `get_chat_session()` - loads messages
- System prompt building with history

**Frontend**:
- Message hydration in `useCopilotPage.ts`
- Check if `useChat` sends full conversation

## Next Steps

1. ✅ My `block=None` fix is correct and tested
2. ❌ Need to investigate WHY _stream_listener was being cancelled in old logs
3. ❌ Need to check if executor is starting and publishing chunks quickly
4. ❌ Need to verify StreamFinish is being sent
5. ❌ Need to check frontend SSE connection stability
6. ❌ Need to investigate conversation history handling

## Key Questions to Answer

1. **Timing**: How long from `enqueue_task` to `first chunk published`?
2. **Delivery**: Are chunks reaching subscriber_queue?
3. **Completion**: Is StreamFinish being sent and delivered?
4. **Context**: Is conversation history being maintained?
5. **Tools**: Why do long-running tools complete but not continue?
