name: AutoGPT Platform - Frontend CI

on:
  workflow_dispatch:
  push:
    branches: [master, dev]
    paths:
      - ".github/workflows/platform-frontend-ci.yml"
      - "autogpt_platform/frontend/**"
  pull_request:
    paths:
      - ".github/workflows/platform-frontend-ci.yml"
      - "autogpt_platform/frontend/**"
  merge_group:

concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'merge_group' && format('merge-queue-{0}', github.ref) || format('{0}-{1}', github.ref, github.event.pull_request.number || github.sha) }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

defaults:
  run:
    shell: bash
    working-directory: autogpt_platform/frontend

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.18.0"

      - name: Enable corepack
        run: corepack enable

      - name: Generate cache key
        id: cache-key
        run: echo "key=${{ runner.os }}-pnpm-${{ hashFiles('autogpt_platform/frontend/pnpm-lock.yaml', 'autogpt_platform/frontend/package.json') }}" >> $GITHUB_OUTPUT

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.pnpm-store
          key: ${{ steps.cache-key.outputs.key }}
          restore-keys: |
            ${{ runner.os }}-pnpm-${{ hashFiles('autogpt_platform/frontend/pnpm-lock.yaml') }}
            ${{ runner.os }}-pnpm-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

  lint:
    runs-on: ubuntu-latest
    needs: setup

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.18.0"

      - name: Enable corepack
        run: corepack enable

      - name: Restore dependencies cache
        uses: actions/cache@v4
        with:
          path: ~/.pnpm-store
          key: ${{ needs.setup.outputs.cache-key }}
          restore-keys: |
            ${{ runner.os }}-pnpm-${{ hashFiles('autogpt_platform/frontend/pnpm-lock.yaml') }}
            ${{ runner.os }}-pnpm-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Run lint
        run: pnpm lint

  chromatic:
    runs-on: ubuntu-latest
    needs: setup
    # Only run on dev branch pushes or PRs targeting dev
    if: github.ref == 'refs/heads/dev' || github.base_ref == 'dev'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.18.0"

      - name: Enable corepack
        run: corepack enable

      - name: Restore dependencies cache
        uses: actions/cache@v4
        with:
          path: ~/.pnpm-store
          key: ${{ needs.setup.outputs.cache-key }}
          restore-keys: |
            ${{ runner.os }}-pnpm-${{ hashFiles('autogpt_platform/frontend/pnpm-lock.yaml') }}
            ${{ runner.os }}-pnpm-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Run Chromatic
        uses: chromaui/action@latest
        with:
          projectToken: chpt_9e7c1a76478c9c8
          onlyChanged: true
          workingDir: autogpt_platform/frontend
          token: ${{ secrets.GITHUB_TOKEN }}
          exitOnceUploaded: true

  test:
    runs-on: ubuntu-latest
    needs: setup
    strategy:
      fail-fast: false

    steps:
      - name: Free disk space
        working-directory: .
        run: |
          echo "=== Disk space before cleanup ==="
          df -h /
          echo ""
          echo "=== Cleaning up Docker resources ==="
          docker system prune -af --volumes || true
          echo ""
          echo "=== Removing old Docker image cache ==="
          rm -rf ~/docker-cache || true
          echo ""
          echo "=== Disk space after cleanup ==="
          df -h /

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.18.0"

      - name: Enable corepack
        run: corepack enable

      - name: Copy default supabase .env
        run: |
          cp ../.env.default ../.env

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Docker image tar caching - loads images from cache in parallel for faster startup
      - name: Set up Docker image cache
        id: docker-cache
        uses: actions/cache@v4
        with:
          path: ~/docker-cache
          key: docker-images-frontend-${{ runner.os }}-${{ hashFiles('autogpt_platform/docker-compose.yml') }}
          restore-keys: |
            docker-images-frontend-${{ runner.os }}-

      - name: Load or pull Docker images
        working-directory: autogpt_platform
        run: |
          mkdir -p ~/docker-cache

          # Define image list for easy maintenance
          IMAGES=(
            "redis:latest"
            "rabbitmq:management"
            "kong:2.8.1"
            "supabase/gotrue:v2.170.0"
            "supabase/postgres:15.8.1.049"
          )

          # Check if any cached tar files exist
          if ls ~/docker-cache/*.tar 1> /dev/null 2>&1; then
            echo "Docker cache found, loading images in parallel..."
            for image in "${IMAGES[@]}"; do
              filename=$(echo "$image" | tr ':/' '--')
              if [ -f ~/docker-cache/${filename}.tar ]; then
                echo "Loading $image..."
                docker load -i ~/docker-cache/${filename}.tar || echo "Warning: Failed to load $image from cache" &
              fi
            done
            wait
            echo "All cached images loaded"
          else
            echo "No Docker cache found, pulling images in parallel..."
            for image in "${IMAGES[@]}"; do
              docker pull "$image" &
            done
            wait

            # Only save cache on main branches (not PRs) to avoid cache pollution
            if [[ "${{ github.ref }}" == "refs/heads/master" ]] || [[ "${{ github.ref }}" == "refs/heads/dev" ]]; then
              echo "Saving Docker images to cache in parallel..."
              for image in "${IMAGES[@]}"; do
                filename=$(echo "$image" | tr ':/' '--')
                echo "Saving $image..."
                docker save -o ~/docker-cache/${filename}.tar "$image" || echo "Warning: Failed to save $image" &
              done
              wait
              echo "Docker image cache saved"
            else
              echo "Skipping cache save for PR/feature branch"
            fi
          fi

          echo "Docker images ready for use"

      - name: Run docker compose
        run: |
          NEXT_PUBLIC_PW_TEST=true docker compose -f ../docker-compose.yml up -d
        env:
          DOCKER_BUILDKIT: 1

      - name: Wait for services to be ready
        run: |
          echo "Waiting for rest_server to be ready..."
          timeout 30 sh -c 'until curl -f http://localhost:8006/health 2>/dev/null; do sleep 2; done' || echo "Rest server health check timeout, continuing..."
          echo "Waiting for database to be ready..."
          timeout 30 sh -c 'until docker compose -f ../docker-compose.yml exec -T db pg_isready -U postgres 2>/dev/null; do sleep 2; done' || echo "Database ready check timeout, continuing..."

      - name: Create E2E test data
        run: |
          echo "Creating E2E test data..."
          # First try to run the script from inside the container
          if docker compose -f ../docker-compose.yml exec -T rest_server test -f /app/autogpt_platform/backend/test/e2e_test_data.py; then
            echo "✅ Found e2e_test_data.py in container, running it..."
            docker compose -f ../docker-compose.yml exec -T rest_server sh -c "cd /app/autogpt_platform && python backend/test/e2e_test_data.py" || {
              echo "❌ E2E test data creation failed!"
              docker compose -f ../docker-compose.yml logs --tail=50 rest_server
              exit 1
            }
          else
            echo "⚠️ e2e_test_data.py not found in container, copying and running..."
            # Copy the script into the container and run it
            docker cp ../backend/test/e2e_test_data.py $(docker compose -f ../docker-compose.yml ps -q rest_server):/tmp/e2e_test_data.py || {
              echo "❌ Failed to copy script to container"
              exit 1
            }
            docker compose -f ../docker-compose.yml exec -T rest_server sh -c "cd /app/autogpt_platform && python /tmp/e2e_test_data.py" || {
              echo "❌ E2E test data creation failed!"
              docker compose -f ../docker-compose.yml logs --tail=50 rest_server
              exit 1
            }
          fi

      - name: Restore dependencies cache
        uses: actions/cache@v4
        with:
          path: ~/.pnpm-store
          key: ${{ needs.setup.outputs.cache-key }}
          restore-keys: |
            ${{ runner.os }}-pnpm-${{ hashFiles('autogpt_platform/frontend/pnpm-lock.yaml') }}
            ${{ runner.os }}-pnpm-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      # Playwright browser caching - saves 30-60s when cache hits
      - name: Get Playwright version
        id: playwright-version
        run: |
          echo "version=$(pnpm list @playwright/test --json | jq -r '.[0].dependencies["@playwright/test"].version')" >> $GITHUB_OUTPUT

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ steps.playwright-version.outputs.version }}

      - name: Install Playwright browsers
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: pnpm playwright install --with-deps chromium

      - name: Install Playwright deps only (when cache hit)
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: pnpm playwright install-deps chromium

      - name: Run Playwright tests
        run: pnpm test:no-build

      - name: Upload Playwright artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: playwright-report

      - name: Print Final Docker Compose logs
        if: always()
        run: docker compose -f ../docker-compose.yml logs

      - name: Cleanup Docker resources
        if: always()
        run: |
          docker compose -f ../docker-compose.yml down -v --remove-orphans || true
          docker system prune -af --volumes || true
