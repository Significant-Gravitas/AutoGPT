# %% [markdown]
# # üß¨ Stanford RNA 3D Folding Part 2 - EDA & Visualisation 3D
#
# Ce notebook explore les donn√©es du concours Stanford RNA 3D Folding Part 2.
#
# **Objectif** : Pr√©dire la structure 3D de mol√©cules d'ARN √† partir de leur s√©quence.
#
# **Sections** :
# 1. Chargement des donn√©es
# 2. Analyse exploratoire des s√©quences
# 3. Analyse des labels (coordonn√©es 3D)
# 4. Visualisation 3D des structures RNA
# 5. Analyse des MSA (Multiple Sequence Alignments)
# 6. Exploration des m√©tadonn√©es

# %% [markdown]
# ## üì¶ 1. Imports et Configuration

# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
from collections import Counter
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-whitegrid')

# ============================================
# Fonction simple pour parser les fichiers FASTA (sans BioPython)
# ============================================
def parse_fasta(fasta_path):
    """Parse un fichier FASTA et retourne une liste de tuples (header, sequence)."""
    sequences = []
    current_header = None
    current_seq = []

    with open(fasta_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith('>'):
                if current_header is not None:
                    sequences.append((current_header, ''.join(current_seq)))
                current_header = line[1:]  # Enlever le '>'
                current_seq = []
            else:
                current_seq.append(line)

        # Ajouter la derni√®re s√©quence
        if current_header is not None:
            sequences.append((current_header, ''.join(current_seq)))

    return sequences

# Configuration des couleurs pour les nucl√©otides
NUCLEOTIDE_COLORS = {
    'A': '#FF6B6B',  # Rouge - Ad√©nine
    'U': '#4ECDC4',  # Cyan - Uracile
    'G': '#45B7D1',  # Bleu - Guanine
    'C': '#96CEB4',  # Vert - Cytosine
}

# Chemin vers les donn√©es (ajuster selon l'environnement Kaggle)
DATA_PATH = Path('/kaggle/input/stanford-rna-3d-folding-2')

print("üìÅ Structure des donn√©es:")
print(f"  - MSA/: {len(list((DATA_PATH / 'MSA').glob('*.fasta')))} fichiers")
print(f"  - PDB_RNA/: {len(list((DATA_PATH / 'PDB_RNA').glob('*.cif')))} fichiers")

# %% [markdown]
# ## üìä 2. Chargement des Donn√©es

# %%
# Chargement des fichiers CSV principaux
train_sequences = pd.read_csv(DATA_PATH / 'train_sequences.csv')
validation_sequences = pd.read_csv(DATA_PATH / 'validation_sequences.csv')
test_sequences = pd.read_csv(DATA_PATH / 'test_sequences.csv')

train_labels = pd.read_csv(DATA_PATH / 'train_labels.csv')
validation_labels = pd.read_csv(DATA_PATH / 'validation_labels.csv')

sample_submission = pd.read_csv(DATA_PATH / 'sample_submission.csv')

# M√©tadonn√©es
rna_metadata = pd.read_csv(DATA_PATH / 'extra' / 'rna_metadata.csv')

print("=" * 60)
print("üìã R√âSUM√â DES DONN√âES")
print("=" * 60)
print(f"\nüîπ S√©quences:")
print(f"   Train:       {len(train_sequences):,} s√©quences")
print(f"   Validation:  {len(validation_sequences):,} s√©quences")
print(f"   Test:        {len(test_sequences):,} s√©quences")
print(f"\nüîπ Labels (coordonn√©es 3D):")
print(f"   Train:       {len(train_labels):,} r√©sidus")
print(f"   Validation:  {len(validation_labels):,} r√©sidus")
print(f"\nüîπ M√©tadonn√©es: {len(rna_metadata):,} entr√©es")

# %% [markdown]
# ## üî¨ 3. Analyse des S√©quences

# %% [markdown]
# ### 3.1 Structure des donn√©es de s√©quences

# %%
print("üìã Colonnes train_sequences:")
print(train_sequences.columns.tolist())
print("\n" + "=" * 60)
train_sequences.head(3)

# %%
# Aper√ßu d√©taill√© d'une s√©quence
print("üîç Exemple de s√©quence (premi√®re entr√©e):")
print("-" * 60)
for col in train_sequences.columns:
    val = train_sequences.iloc[0][col]
    if isinstance(val, str) and len(val) > 100:
        print(f"{col}: {val[:100]}...")
    else:
        print(f"{col}: {val}")

# %% [markdown]
# ### 3.2 Distribution des longueurs de s√©quences

# %%
# Calculer les longueurs
train_sequences['seq_length'] = train_sequences['sequence'].str.len()
validation_sequences['seq_length'] = validation_sequences['sequence'].str.len()
test_sequences['seq_length'] = test_sequences['sequence'].str.len()

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Train
axes[0].hist(train_sequences['seq_length'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')
axes[0].set_title(f'Train (n={len(train_sequences):,})', fontsize=12, fontweight='bold')
axes[0].set_xlabel('Longueur de s√©quence')
axes[0].set_ylabel('Fr√©quence')
axes[0].axvline(train_sequences['seq_length'].median(), color='red', linestyle='--', label=f"M√©diane: {train_sequences['seq_length'].median():.0f}")
axes[0].legend()

# Validation
axes[1].hist(validation_sequences['seq_length'], bins=30, color='seagreen', alpha=0.7, edgecolor='black')
axes[1].set_title(f'Validation (n={len(validation_sequences):,})', fontsize=12, fontweight='bold')
axes[1].set_xlabel('Longueur de s√©quence')
axes[1].axvline(validation_sequences['seq_length'].median(), color='red', linestyle='--', label=f"M√©diane: {validation_sequences['seq_length'].median():.0f}")
axes[1].legend()

# Test
axes[2].hist(test_sequences['seq_length'], bins=30, color='coral', alpha=0.7, edgecolor='black')
axes[2].set_title(f'Test (n={len(test_sequences):,})', fontsize=12, fontweight='bold')
axes[2].set_xlabel('Longueur de s√©quence')
axes[2].axvline(test_sequences['seq_length'].median(), color='red', linestyle='--', label=f"M√©diane: {test_sequences['seq_length'].median():.0f}")
axes[2].legend()

plt.suptitle('üìä Distribution des Longueurs de S√©quences RNA', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

# Statistiques
print("\nüìà Statistiques des longueurs:")
print("-" * 60)
for name, df in [('Train', train_sequences), ('Validation', validation_sequences), ('Test', test_sequences)]:
    print(f"{name:12} | Min: {df['seq_length'].min():5} | Max: {df['seq_length'].max():5} | "
          f"Mean: {df['seq_length'].mean():7.1f} | Median: {df['seq_length'].median():6.0f}")

# %% [markdown]
# ### 3.3 Composition en nucl√©otides

# %%
def analyze_nucleotide_composition(sequences, name):
    """Analyse la composition en nucl√©otides d'un ensemble de s√©quences."""
    all_nucleotides = ''.join(sequences['sequence'].values)
    counts = Counter(all_nucleotides)
    total = sum(counts.values())

    composition = {nt: (count / total) * 100 for nt, count in counts.items()}
    return composition

# Analyser chaque dataset
compositions = {}
for name, df in [('Train', train_sequences), ('Validation', validation_sequences), ('Test', test_sequences)]:
    compositions[name] = analyze_nucleotide_composition(df, name)

# Visualisation
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, (name, comp) in enumerate(compositions.items()):
    nucleotides = ['A', 'U', 'G', 'C']
    values = [comp.get(nt, 0) for nt in nucleotides]
    colors = [NUCLEOTIDE_COLORS[nt] for nt in nucleotides]

    bars = axes[idx].bar(nucleotides, values, color=colors, edgecolor='black', linewidth=1.5)
    axes[idx].set_title(f'{name}', fontsize=12, fontweight='bold')
    axes[idx].set_ylabel('Pourcentage (%)')
    axes[idx].set_ylim(0, 35)

    # Ajouter les valeurs sur les barres
    for bar, val in zip(bars, values):
        axes[idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                      f'{val:.1f}%', ha='center', fontsize=10, fontweight='bold')

plt.suptitle('üß¨ Composition en Nucl√©otides (A, U, G, C)', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

# Autres nucl√©otides (modifications)
print("\nüî¨ Nucl√©otides non-canoniques d√©tect√©s:")
for name, comp in compositions.items():
    non_canonical = {k: v for k, v in comp.items() if k not in ['A', 'U', 'G', 'C']}
    if non_canonical:
        print(f"  {name}: {non_canonical}")
    else:
        print(f"  {name}: Aucun")

# %% [markdown]
# ### 3.4 Distribution temporelle (temporal_cutoff)

# %%
# Convertir les dates
train_sequences['temporal_cutoff'] = pd.to_datetime(train_sequences['temporal_cutoff'])
validation_sequences['temporal_cutoff'] = pd.to_datetime(validation_sequences['temporal_cutoff'])

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Train
train_sequences.groupby(train_sequences['temporal_cutoff'].dt.to_period('M')).size().plot(
    kind='bar', ax=axes[0], color='steelblue', alpha=0.7
)
axes[0].set_title('Train - Distribution temporelle', fontsize=12, fontweight='bold')
axes[0].set_xlabel('Date de publication')
axes[0].set_ylabel('Nombre de structures')
axes[0].tick_params(axis='x', rotation=45)

# Validation
validation_sequences.groupby(validation_sequences['temporal_cutoff'].dt.to_period('M')).size().plot(
    kind='bar', ax=axes[1], color='seagreen', alpha=0.7
)
axes[1].set_title('Validation - Distribution temporelle', fontsize=12, fontweight='bold')
axes[1].set_xlabel('Date de publication')
axes[1].set_ylabel('Nombre de structures')
axes[1].tick_params(axis='x', rotation=45)

plt.suptitle('üìÖ Distribution Temporelle des Structures', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

print(f"\nüìÖ Plages temporelles:")
print(f"  Train:      {train_sequences['temporal_cutoff'].min().date()} ‚Üí {train_sequences['temporal_cutoff'].max().date()}")
print(f"  Validation: {validation_sequences['temporal_cutoff'].min().date()} ‚Üí {validation_sequences['temporal_cutoff'].max().date()}")

# %% [markdown]
# ### 3.5 Analyse de la stoichiom√©trie

# %%
# Analyser les patterns de stoichiom√©trie
def parse_stoichiometry(stoich_str):
    """Parse stoichiometry string to extract chain counts."""
    if pd.isna(stoich_str):
        return {}
    chains = {}
    for part in stoich_str.split(';'):
        if ':' in part:
            chain, count = part.split(':')
            chains[chain.strip()] = int(count)
    return chains

train_sequences['n_chains'] = train_sequences['stoichiometry'].apply(
    lambda x: sum(parse_stoichiometry(x).values()) if pd.notna(x) else 0
)
validation_sequences['n_chains'] = validation_sequences['stoichiometry'].apply(
    lambda x: sum(parse_stoichiometry(x).values()) if pd.notna(x) else 0
)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Distribution du nombre de cha√Ænes
for idx, (name, df, color) in enumerate([('Train', train_sequences, 'steelblue'),
                                          ('Validation', validation_sequences, 'seagreen')]):
    chain_counts = df['n_chains'].value_counts().sort_index()
    axes[idx].bar(chain_counts.index, chain_counts.values, color=color, alpha=0.7, edgecolor='black')
    axes[idx].set_title(f'{name} - Nombre de cha√Ænes', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Nombre de cha√Ænes')
    axes[idx].set_ylabel('Fr√©quence')

plt.suptitle('üîó Distribution du Nombre de Cha√Ænes par Structure', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

print("\nüìä Statistiques sur les cha√Ænes:")
print(f"  Train - Moyenne: {train_sequences['n_chains'].mean():.2f}, Max: {train_sequences['n_chains'].max()}")
print(f"  Validation - Moyenne: {validation_sequences['n_chains'].mean():.2f}, Max: {validation_sequences['n_chains'].max()}")

# %% [markdown]
# ## üéØ 4. Analyse des Labels (Coordonn√©es 3D)

# %% [markdown]
# ### 4.1 Structure des labels

# %%
print("üìã Colonnes train_labels:")
print(train_labels.columns.tolist())
print("\n" + "=" * 60)
train_labels.head(10)

# %%
# Nombre de structures alternatives par cible
coord_cols = [c for c in train_labels.columns if c.startswith('x_')]
n_structures = len(coord_cols)
print(f"\nüîπ Nombre de structures alternatives dans train_labels: {n_structures}")
print(f"   Colonnes de coordonn√©es: {coord_cols}")

# V√©rifier les valeurs manquantes
print(f"\nüîπ Valeurs manquantes dans les coordonn√©es:")
for col in ['x_1', 'y_1', 'z_1']:
    missing = train_labels[col].isna().sum()
    print(f"   {col}: {missing:,} ({missing/len(train_labels)*100:.2f}%)")

# %% [markdown]
# ### 4.2 Distribution des r√©sidus par structure

# %%
# Compter les r√©sidus par target_id
train_labels['target_id'] = train_labels['ID'].str.rsplit('_', n=1).str[0]
residues_per_target = train_labels.groupby('target_id').size()

fig, ax = plt.subplots(figsize=(12, 5))
ax.hist(residues_per_target, bins=50, color='purple', alpha=0.7, edgecolor='black')
ax.set_title('Distribution du Nombre de R√©sidus par Structure', fontsize=12, fontweight='bold')
ax.set_xlabel('Nombre de r√©sidus')
ax.set_ylabel('Fr√©quence')
ax.axvline(residues_per_target.median(), color='red', linestyle='--',
           label=f"M√©diane: {residues_per_target.median():.0f}")
ax.legend()
plt.tight_layout()
plt.show()

print(f"\nüìà Statistiques des r√©sidus par structure:")
print(f"   Min: {residues_per_target.min()}, Max: {residues_per_target.max()}")
print(f"   Mean: {residues_per_target.mean():.1f}, Median: {residues_per_target.median():.0f}")

# %% [markdown]
# ### 4.3 Distribution des types de r√©sidus

# %%
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

for idx, (name, df, color) in enumerate([('Train', train_labels, 'steelblue'),
                                          ('Validation', validation_labels, 'seagreen')]):
    resname_counts = df['resname'].value_counts()
    colors = [NUCLEOTIDE_COLORS.get(nt, 'gray') for nt in resname_counts.index]

    bars = axes[idx].bar(resname_counts.index, resname_counts.values, color=colors, edgecolor='black')
    axes[idx].set_title(f'{name} - Distribution des r√©sidus', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Type de r√©sidu')
    axes[idx].set_ylabel('Fr√©quence')

    # Ajouter pourcentages
    total = resname_counts.sum()
    for bar, val in zip(bars, resname_counts.values):
        pct = val / total * 100
        axes[idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + total*0.01,
                      f'{pct:.1f}%', ha='center', fontsize=9)

plt.suptitle('üß¨ Distribution des Types de R√©sidus', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

# %% [markdown]
# ### 4.4 Distribution spatiale des coordonn√©es

# %%
# √âchantillonner pour la visualisation
sample_labels = train_labels.dropna(subset=['x_1', 'y_1', 'z_1']).sample(min(50000, len(train_labels)))

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, (coord, color) in enumerate([('x_1', 'red'), ('y_1', 'green'), ('z_1', 'blue')]):
    axes[idx].hist(sample_labels[coord], bins=50, color=color, alpha=0.7, edgecolor='black')
    axes[idx].set_title(f'Distribution {coord.upper()}', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel(f'{coord} (√Öngstr√∂ms)')
    axes[idx].set_ylabel('Fr√©quence')

    mean_val = sample_labels[coord].mean()
    axes[idx].axvline(mean_val, color='black', linestyle='--', label=f'Mean: {mean_val:.1f}√Ö')
    axes[idx].legend()

plt.suptitle('üìê Distribution Spatiale des Coordonn√©es C1\'', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

# %% [markdown]
# ## üåê 5. Visualisation 3D des Structures RNA

# %% [markdown]
# ### 5.1 Fonction de visualisation 3D

# %%
def visualize_rna_3d(labels_df, target_id, structure_idx=1, title=None):
    """
    Visualise une structure RNA en 3D avec Plotly.

    Args:
        labels_df: DataFrame des labels
        target_id: ID de la cible √† visualiser
        structure_idx: Index de la structure (1, 2, etc.)
        title: Titre personnalis√©
    """
    # Filtrer pour cette cible
    target_data = labels_df[labels_df['ID'].str.startswith(target_id + '_')].copy()
    target_data = target_data.sort_values('resid')

    # Colonnes de coordonn√©es
    x_col, y_col, z_col = f'x_{structure_idx}', f'y_{structure_idx}', f'z_{structure_idx}'

    # V√©rifier que les colonnes existent
    if x_col not in target_data.columns:
        print(f"Structure {structure_idx} non disponible pour {target_id}")
        return None

    # Retirer les valeurs manquantes
    target_data = target_data.dropna(subset=[x_col, y_col, z_col])

    if len(target_data) == 0:
        print(f"Pas de donn√©es pour {target_id}")
        return None

    # Couleurs par nucl√©otide
    colors = [NUCLEOTIDE_COLORS.get(res, 'gray') for res in target_data['resname']]

    # Cr√©er la figure
    fig = go.Figure()

    # Ajouter la trace du backbone (ligne)
    fig.add_trace(go.Scatter3d(
        x=target_data[x_col],
        y=target_data[y_col],
        z=target_data[z_col],
        mode='lines',
        line=dict(color='lightgray', width=3),
        name='Backbone',
        hoverinfo='skip'
    ))

    # Ajouter les points (atomes C1')
    fig.add_trace(go.Scatter3d(
        x=target_data[x_col],
        y=target_data[y_col],
        z=target_data[z_col],
        mode='markers',
        marker=dict(
            size=6,
            color=colors,
            opacity=0.9,
            line=dict(color='black', width=0.5)
        ),
        text=[f"Res {row['resid']}: {row['resname']}<br>({row[x_col]:.2f}, {row[y_col]:.2f}, {row[z_col]:.2f})"
              for _, row in target_data.iterrows()],
        hoverinfo='text',
        name="R√©sidus C1'"
    ))

    # Mise en page
    title_text = title or f"Structure 3D: {target_id} (n={len(target_data)} r√©sidus)"
    fig.update_layout(
        title=dict(text=title_text, font=dict(size=16)),
        scene=dict(
            xaxis_title='X (√Ö)',
            yaxis_title='Y (√Ö)',
            zaxis_title='Z (√Ö)',
            aspectmode='data'
        ),
        width=800,
        height=600,
        showlegend=True,
        legend=dict(x=0.02, y=0.98)
    )

    # Ajouter l√©gende des couleurs
    for nt, color in NUCLEOTIDE_COLORS.items():
        fig.add_trace(go.Scatter3d(
            x=[None], y=[None], z=[None],
            mode='markers',
            marker=dict(size=10, color=color),
            name=f'{nt}',
            showlegend=True
        ))

    return fig

# %% [markdown]
# ### 5.2 Visualisation de structures exemple

# %%
# S√©lectionner quelques structures int√©ressantes
target_ids = train_labels['ID'].str.rsplit('_', n=1).str[0].unique()

# Trouver des structures de diff√©rentes tailles
sizes = train_labels.groupby(train_labels['ID'].str.rsplit('_', n=1).str[0]).size()
small_target = sizes[sizes < 50].index[0] if len(sizes[sizes < 50]) > 0 else sizes.index[0]
medium_target = sizes[(sizes >= 50) & (sizes < 150)].index[0] if len(sizes[(sizes >= 50) & (sizes < 150)]) > 0 else sizes.index[1]
large_target = sizes[sizes >= 150].index[0] if len(sizes[sizes >= 150]) > 0 else sizes.index[2]

print(f"üîç Structures s√©lectionn√©es pour visualisation:")
print(f"   Petite:  {small_target} ({sizes[small_target]} r√©sidus)")
print(f"   Moyenne: {medium_target} ({sizes[medium_target]} r√©sidus)")
print(f"   Grande:  {large_target} ({sizes[large_target]} r√©sidus)")

# %%
# Visualiser une petite structure
fig = visualize_rna_3d(train_labels, small_target)
if fig:
    fig.show()

# %%
# Visualiser une structure moyenne
fig = visualize_rna_3d(train_labels, medium_target)
if fig:
    fig.show()

# %%
# Visualiser une grande structure
fig = visualize_rna_3d(train_labels, large_target)
if fig:
    fig.show()

# %% [markdown]
# ### 5.3 Comparaison de structures alternatives (si disponibles)

# %%
def compare_structures(labels_df, target_id, n_structures=2):
    """Compare plusieurs structures alternatives pour une m√™me cible."""
    target_data = labels_df[labels_df['ID'].str.startswith(target_id + '_')].copy()
    target_data = target_data.sort_values('resid')

    # V√©rifier combien de structures sont disponibles
    x_cols = [c for c in target_data.columns if c.startswith('x_')]
    n_available = len(x_cols)

    if n_available < 2:
        print(f"Seulement {n_available} structure(s) disponible(s) pour {target_id}")
        return None

    n_to_show = min(n_structures, n_available)

    fig = go.Figure()
    colors = ['blue', 'red', 'green', 'orange', 'purple']

    for i in range(1, n_to_show + 1):
        x_col, y_col, z_col = f'x_{i}', f'y_{i}', f'z_{i}'
        data = target_data.dropna(subset=[x_col, y_col, z_col])

        if len(data) > 0:
            fig.add_trace(go.Scatter3d(
                x=data[x_col],
                y=data[y_col],
                z=data[z_col],
                mode='lines+markers',
                marker=dict(size=4, opacity=0.7),
                line=dict(width=2),
                name=f'Structure {i}',
                marker_color=colors[i-1]
            ))

    fig.update_layout(
        title=f"Comparaison des structures: {target_id}",
        scene=dict(
            xaxis_title='X (√Ö)',
            yaxis_title='Y (√Ö)',
            zaxis_title='Z (√Ö)',
            aspectmode='data'
        ),
        width=800,
        height=600
    )

    return fig

# Trouver une cible avec plusieurs structures
multi_struct_cols = [c for c in train_labels.columns if c.startswith('x_')]
if len(multi_struct_cols) > 1:
    # V√©rifier quelles cibles ont plusieurs conformations
    sample_target = train_labels['ID'].str.rsplit('_', n=1).str[0].iloc[0]
    fig = compare_structures(train_labels, sample_target)
    if fig:
        fig.show()

# %% [markdown]
# ### 5.4 Analyse des distances inter-r√©sidus

# %%
def analyze_distances(labels_df, target_id, structure_idx=1):
    """Analyse les distances entre r√©sidus cons√©cutifs."""
    target_data = labels_df[labels_df['ID'].str.startswith(target_id + '_')].copy()
    target_data = target_data.sort_values('resid')

    x_col, y_col, z_col = f'x_{structure_idx}', f'y_{structure_idx}', f'z_{structure_idx}'
    target_data = target_data.dropna(subset=[x_col, y_col, z_col])

    if len(target_data) < 2:
        return None

    # Calculer les distances cons√©cutives
    coords = target_data[[x_col, y_col, z_col]].values
    distances = np.sqrt(np.sum(np.diff(coords, axis=0)**2, axis=1))

    return distances

# Calculer pour plusieurs structures
all_distances = []
sample_targets = train_labels['ID'].str.rsplit('_', n=1).str[0].unique()[:100]

for target in sample_targets:
    distances = analyze_distances(train_labels, target)
    if distances is not None:
        all_distances.extend(distances)

all_distances = np.array(all_distances)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Histogramme
axes[0].hist(all_distances, bins=50, color='purple', alpha=0.7, edgecolor='black')
axes[0].set_title('Distribution des Distances C1\'-C1\' Cons√©cutives', fontsize=12, fontweight='bold')
axes[0].set_xlabel('Distance (√Ö)')
axes[0].set_ylabel('Fr√©quence')
axes[0].axvline(np.median(all_distances), color='red', linestyle='--',
                label=f'M√©diane: {np.median(all_distances):.2f}√Ö')
axes[0].legend()

# Box plot
axes[1].boxplot(all_distances, vert=True)
axes[1].set_title('Box Plot des Distances', fontsize=12, fontweight='bold')
axes[1].set_ylabel('Distance (√Ö)')

plt.suptitle('üìè Analyse des Distances Inter-R√©sidus', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

print(f"\nüìä Statistiques des distances C1'-C1' cons√©cutives:")
print(f"   Mean: {np.mean(all_distances):.2f}√Ö")
print(f"   Median: {np.median(all_distances):.2f}√Ö")
print(f"   Std: {np.std(all_distances):.2f}√Ö")
print(f"   Min: {np.min(all_distances):.2f}√Ö, Max: {np.max(all_distances):.2f}√Ö")

# %% [markdown]
# ## üìÇ 6. Analyse des MSA (Multiple Sequence Alignments)

# %%
def analyze_msa(msa_path, n_samples=5):
    """Analyse un fichier MSA."""
    try:
        sequences = parse_fasta(msa_path)

        info = {
            'n_sequences': len(sequences),
            'target_length': len(sequences[0][1]) if sequences else 0,
            'headers': [seq[0] for seq in sequences[:n_samples]]
        }

        return info
    except Exception as e:
        return {'error': str(e)}

# Analyser quelques MSA
msa_dir = DATA_PATH / 'MSA'
msa_files = list(msa_dir.glob('*.fasta'))[:10]

print(f"üìÅ Analyse de {len(msa_files)} fichiers MSA (sur {len(list(msa_dir.glob('*.fasta')))} total):\n")

msa_stats = []
for msa_file in msa_files:
    info = analyze_msa(msa_file)
    msa_stats.append({
        'file': msa_file.name,
        'n_sequences': info.get('n_sequences', 0),
        'target_length': info.get('target_length', 0)
    })
    print(f"  {msa_file.name}: {info.get('n_sequences', 'N/A')} s√©quences, longueur {info.get('target_length', 'N/A')}")

# %%
# Distribution de la profondeur des MSA
all_msa_files = list(msa_dir.glob('*.fasta'))
msa_depths = []

print(f"\n‚è≥ Analyse de la profondeur des MSA ({len(all_msa_files)} fichiers)...")

for i, msa_file in enumerate(all_msa_files[:500]):  # Limiter pour la vitesse
    try:
        sequences = parse_fasta(msa_file)
        msa_depths.append(len(sequences))
    except:
        pass

fig, ax = plt.subplots(figsize=(10, 5))
ax.hist(msa_depths, bins=50, color='teal', alpha=0.7, edgecolor='black')
ax.set_title('Distribution de la Profondeur des MSA', fontsize=12, fontweight='bold')
ax.set_xlabel('Nombre de s√©quences dans le MSA')
ax.set_ylabel('Fr√©quence')
ax.axvline(np.median(msa_depths), color='red', linestyle='--', label=f'M√©diane: {np.median(msa_depths):.0f}')
ax.legend()
plt.tight_layout()
plt.show()

print(f"\nüìä Statistiques de profondeur MSA:")
print(f"   Min: {np.min(msa_depths)}, Max: {np.max(msa_depths)}")
print(f"   Mean: {np.mean(msa_depths):.1f}, Median: {np.median(msa_depths):.0f}")

# %% [markdown]
# ## üìã 7. Analyse des M√©tadonn√©es

# %%
print("üìã Colonnes rna_metadata:")
print(rna_metadata.columns.tolist())
print(f"\nShape: {rna_metadata.shape}")
rna_metadata.head()

# %%
# Analyse des colonnes cl√©s
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Resolution
if 'resolution' in rna_metadata.columns:
    rna_metadata['resolution'].dropna().hist(bins=50, ax=axes[0, 0], color='steelblue', alpha=0.7)
    axes[0, 0].set_title('Distribution de la R√©solution', fontsize=11, fontweight='bold')
    axes[0, 0].set_xlabel('R√©solution (√Ö)')

# Method
if 'method' in rna_metadata.columns:
    method_counts = rna_metadata['method'].value_counts().head(10)
    method_counts.plot(kind='barh', ax=axes[0, 1], color='seagreen', alpha=0.7)
    axes[0, 1].set_title('M√©thodes Exp√©rimentales', fontsize=11, fontweight='bold')
    axes[0, 1].set_xlabel('Nombre de structures')

# RNA composition
if 'rna_composition' in rna_metadata.columns:
    rna_metadata['rna_composition'].dropna().hist(bins=50, ax=axes[1, 0], color='coral', alpha=0.7)
    axes[1, 0].set_title('Composition RNA (%)', fontsize=11, fontweight='bold')
    axes[1, 0].set_xlabel('% RNA')

# Structuredness
if 'structuredness' in rna_metadata.columns:
    rna_metadata['structuredness'].dropna().hist(bins=50, ax=axes[1, 1], color='purple', alpha=0.7)
    axes[1, 1].set_title('Structuredness', fontsize=11, fontweight='bold')
    axes[1, 1].set_xlabel('Structuredness score')

plt.suptitle('üìä Analyse des M√©tadonn√©es RNA', fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

# %% [markdown]
# ## üìù 8. Analyse du Format de Soumission

# %%
print("üìã Format de soumission:")
print(sample_submission.columns.tolist())
print(f"\nShape: {sample_submission.shape}")
sample_submission.head(10)

# %%
# V√©rifier le nombre de pr√©dictions requises
coord_cols = [c for c in sample_submission.columns if c.startswith('x_')]
print(f"\nüéØ Nombre de structures √† pr√©dire: {len(coord_cols)} (x_1 √† x_{len(coord_cols)})")

# Nombre de cibles uniques dans le test
test_target_ids = sample_submission['ID'].str.rsplit('_', n=1).str[0].unique()
print(f"üìä Nombre de cibles test: {len(test_target_ids)}")
print(f"üìä Nombre total de r√©sidus √† pr√©dire: {len(sample_submission)}")

# %% [markdown]
# ## üéØ 9. R√©sum√© et Insights Cl√©s

# %%
print("=" * 70)
print("                    üìä R√âSUM√â DE L'ANALYSE EDA")
print("=" * 70)

print("""
üîπ DONN√âES:
   ‚Ä¢ Train: {train_seq} s√©quences, {train_res:,} r√©sidus
   ‚Ä¢ Validation: {val_seq} s√©quences, {val_res:,} r√©sidus
   ‚Ä¢ Test: {test_seq} s√©quences √† pr√©dire
   ‚Ä¢ MSA disponibles: {msa_count:,} fichiers
   ‚Ä¢ Structures PDB: {pdb_count:,} fichiers

üîπ S√âQUENCES:
   ‚Ä¢ Longueur m√©diane: Train={train_med:.0f}, Val={val_med:.0f}, Test={test_med:.0f}
   ‚Ä¢ Composition: ~25% chaque nucl√©otide (A, U, G, C √©quilibr√©s)
   ‚Ä¢ Structures multi-cha√Ænes pr√©sentes

üîπ STRUCTURES 3D:
   ‚Ä¢ Coordonn√©es: Position de l'atome C1' de chaque r√©sidu
   ‚Ä¢ Distance C1'-C1' cons√©cutive: ~{dist_mean:.1f}√Ö (m√©diane)
   ‚Ä¢ Format soumission: 5 structures par cible

üîπ INSIGHTS POUR LA MOD√âLISATION:
   ‚Ä¢ Les MSA profonds peuvent aider (info √©volutive)
   ‚Ä¢ Templates PDB disponibles pour recherche d'homologues
   ‚Ä¢ Validation temporelle: structures apr√®s mai 2025
   ‚Ä¢ M√©trique: TM-score (best of 5 predictions)
""".format(
    train_seq=len(train_sequences),
    train_res=len(train_labels),
    val_seq=len(validation_sequences),
    val_res=len(validation_labels),
    test_seq=len(test_sequences),
    msa_count=len(list((DATA_PATH / 'MSA').glob('*.fasta'))),
    pdb_count=len(list((DATA_PATH / 'PDB_RNA').glob('*.cif'))),
    train_med=train_sequences['seq_length'].median(),
    val_med=validation_sequences['seq_length'].median(),
    test_med=test_sequences['seq_length'].median(),
    dist_mean=np.median(all_distances) if len(all_distances) > 0 else 5.9
))

print("=" * 70)
print("                    üöÄ PR√äT POUR LA MOD√âLISATION!")
print("=" * 70)

# %% [markdown]
# ---
# # üß¨ PARTIE 2 : STRAT√âGIES DE MOD√âLISATION
# ---
#
# Bas√© sur les solutions gagnantes de la Part 1 :
# - **1√®re place (john)** : Template-Based Modeling (TBM) pur
# - **2√®me place (odat)** : TBM avec alignement optimis√©
# - **3√®me place (Eigen)** : Hybride TBM + Deep Learning
#
# ## Approche retenue : Template-Based Modeling
# 95% des cibles ont des templates potentiels dans le PDB !

# %% [markdown]
# ## üîç 10. Pipeline Template-Based Modeling (TBM)

# %%
# ============================================
# STRAT√âGIE 1 : TEMPLATE-BASED MODELING
# ============================================
# Pipeline :
# 1. Parser les s√©quences du PDB_RNA
# 2. Aligner la s√©quence cible avec les s√©quences PDB
# 3. Trouver les meilleurs templates
# 4. Copier/adapter les coordonn√©es 3D
# 5. G√©n√©rer 5 pr√©dictions diversifi√©es

print("üîß Configuration du pipeline TBM...")

# %% [markdown]
# ### 10.1 Extraction des s√©quences du PDB

# %%
from gemmi import cif  # Parser CIF rapide (disponible sur Kaggle)

def extract_rna_sequences_from_cif(cif_path):
    """
    Extrait les s√©quences RNA et coordonn√©es C1' d'un fichier CIF.
    Retourne un dict avec les infos de la structure.
    """
    try:
        doc = cif.read(str(cif_path))
        block = doc.sole_block()

        # Extraire les coordonn√©es des atomes
        atom_site = block.find('_atom_site.',
            ['label_atom_id', 'label_comp_id', 'label_asym_id',
             'label_seq_id', 'Cartn_x', 'Cartn_y', 'Cartn_z',
             'type_symbol', 'group_PDB'])

        if not atom_site:
            return None

        residues = {}
        coords = {}

        for row in atom_site:
            atom_name = row[0]
            res_name = row[1]
            chain_id = row[2]
            seq_id = row[3]

            # Filtrer uniquement les nucl√©otides RNA (A, U, G, C)
            if res_name in ['A', 'U', 'G', 'C', 'ADE', 'URA', 'GUA', 'CYT']:
                # Mapper les noms longs vers courts
                res_short = {'ADE': 'A', 'URA': 'U', 'GUA': 'G', 'CYT': 'C'}.get(res_name, res_name)

                key = (chain_id, seq_id)
                if key not in residues:
                    residues[key] = res_short

                # Stocker les coordonn√©es C1'
                if atom_name == "C1'":
                    try:
                        x, y, z = float(row[4]), float(row[5]), float(row[6])
                        coords[key] = (x, y, z)
                    except:
                        pass

        # Construire la s√©quence par cha√Æne
        chains = {}
        for (chain_id, seq_id), res in sorted(residues.items()):
            if chain_id not in chains:
                chains[chain_id] = {'sequence': '', 'coords': []}
            chains[chain_id]['sequence'] += res
            if (chain_id, seq_id) in coords:
                chains[chain_id]['coords'].append(coords[(chain_id, seq_id)])
            else:
                chains[chain_id]['coords'].append(None)

        return {
            'pdb_id': cif_path.stem.upper(),
            'chains': chains
        }

    except Exception as e:
        return None

# Test sur quelques fichiers
print("üìÇ Test d'extraction sur quelques fichiers CIF...")
pdb_dir = DATA_PATH / 'PDB_RNA'
sample_cifs = list(pdb_dir.glob('*.cif'))[:5]

for cif_file in sample_cifs:
    result = extract_rna_sequences_from_cif(cif_file)
    if result:
        for chain_id, chain_data in result['chains'].items():
            seq = chain_data['sequence']
            n_coords = sum(1 for c in chain_data['coords'] if c is not None)
            print(f"  {result['pdb_id']}_{chain_id}: {len(seq)} nt, {n_coords} coords C1'")
            if len(seq) < 50:
                print(f"    S√©quence: {seq}")

# %% [markdown]
# ### 10.2 Construction de la base de templates

# %%
def build_template_database(pdb_dir, max_files=None, verbose=True):
    """
    Construit une base de donn√©es de templates √† partir des fichiers CIF.
    """
    templates = []
    cif_files = list(pdb_dir.glob('*.cif'))

    if max_files:
        cif_files = cif_files[:max_files]

    if verbose:
        print(f"üì¶ Construction de la base de templates ({len(cif_files)} fichiers)...")

    for i, cif_file in enumerate(cif_files):
        if verbose and i % 500 == 0:
            print(f"   Progression: {i}/{len(cif_files)}")

        result = extract_rna_sequences_from_cif(cif_file)
        if result:
            for chain_id, chain_data in result['chains'].items():
                seq = chain_data['sequence']
                coords = chain_data['coords']

                # Filtrer les cha√Ænes trop courtes ou sans coordonn√©es
                n_valid_coords = sum(1 for c in coords if c is not None)
                if len(seq) >= 10 and n_valid_coords >= len(seq) * 0.5:
                    templates.append({
                        'pdb_id': result['pdb_id'],
                        'chain_id': chain_id,
                        'sequence': seq,
                        'coords': coords,
                        'length': len(seq)
                    })

    if verbose:
        print(f"‚úÖ {len(templates)} templates extraits")

    return templates

# Construire une base de templates (limiter pour la d√©mo)
# En production, utiliser tous les fichiers (max_files=None)
templates_db = build_template_database(pdb_dir, max_files=500, verbose=True)

# Statistiques
template_lengths = [t['length'] for t in templates_db]
print(f"\nüìä Statistiques des templates:")
print(f"   Nombre: {len(templates_db)}")
print(f"   Longueur min: {min(template_lengths)}, max: {max(template_lengths)}")
print(f"   Longueur moyenne: {np.mean(template_lengths):.1f}")

# %% [markdown]
# ### 10.3 Alignement de s√©quences (Simple)

# %%
def simple_sequence_alignment(seq1, seq2):
    """
    Alignement simple bas√© sur la similarit√© de s√©quence.
    Retourne le score de similarit√© (0-1) et les positions align√©es.
    """
    # M√©thode simple : recherche de sous-cha√Ænes communes
    len1, len2 = len(seq1), len(seq2)

    if len1 == 0 or len2 == 0:
        return 0.0, []

    # Construire une matrice de correspondance
    matches = 0
    aligned_positions = []

    # Alignement global simple (sans gaps pour commencer)
    min_len = min(len1, len2)

    # Essayer diff√©rents d√©calages
    best_score = 0
    best_offset = 0

    for offset in range(-len2 + 1, len1):
        score = 0
        for i in range(min_len):
            pos1 = i
            pos2 = i - offset
            if 0 <= pos1 < len1 and 0 <= pos2 < len2:
                if seq1[pos1] == seq2[pos2]:
                    score += 1

        if score > best_score:
            best_score = score
            best_offset = offset

    # Reconstruire l'alignement avec le meilleur offset
    for i in range(max(len1, len2)):
        pos1 = i
        pos2 = i - best_offset
        if 0 <= pos1 < len1 and 0 <= pos2 < len2:
            if seq1[pos1] == seq2[pos2]:
                aligned_positions.append((pos1, pos2))

    # Score normalis√©
    similarity = best_score / max(len1, len2)

    return similarity, aligned_positions

def find_best_templates(query_seq, templates_db, top_k=5):
    """
    Trouve les meilleurs templates pour une s√©quence query.
    """
    scores = []

    for template in templates_db:
        similarity, aligned_pos = simple_sequence_alignment(query_seq, template['sequence'])

        # Bonus pour longueur similaire
        len_ratio = min(len(query_seq), template['length']) / max(len(query_seq), template['length'])

        combined_score = similarity * 0.7 + len_ratio * 0.3

        scores.append({
            'template': template,
            'similarity': similarity,
            'len_ratio': len_ratio,
            'combined_score': combined_score,
            'aligned_positions': aligned_pos
        })

    # Trier par score combin√©
    scores.sort(key=lambda x: x['combined_score'], reverse=True)

    return scores[:top_k]

# Test sur une s√©quence de validation
print("üîç Test de recherche de templates...")
test_seq = validation_sequences.iloc[0]['sequence']
print(f"   S√©quence test: {test_seq[:50]}... (longueur: {len(test_seq)})")

best_templates = find_best_templates(test_seq, templates_db, top_k=5)

print(f"\nüìã Top 5 templates:")
for i, match in enumerate(best_templates):
    t = match['template']
    print(f"   {i+1}. {t['pdb_id']}_{t['chain_id']}: "
          f"sim={match['similarity']:.3f}, len={t['length']}, "
          f"score={match['combined_score']:.3f}")

# %% [markdown]
# ### 10.4 Pr√©diction de structure par template

# %%
def predict_structure_from_template(query_seq, template_match, noise_std=0.0):
    """
    Pr√©dit les coordonn√©es 3D en utilisant un template.

    Args:
        query_seq: S√©quence cible
        template_match: R√©sultat de find_best_templates
        noise_std: √âcart-type du bruit √† ajouter (pour diversification)

    Returns:
        Liste de coordonn√©es (x, y, z) pour chaque r√©sidu
    """
    template = template_match['template']
    aligned_positions = template_match['aligned_positions']
    template_coords = template['coords']

    # Initialiser les coordonn√©es pr√©dites
    predicted_coords = [(0.0, 0.0, 0.0)] * len(query_seq)

    # Copier les coordonn√©es des positions align√©es
    for query_pos, template_pos in aligned_positions:
        if template_pos < len(template_coords) and template_coords[template_pos] is not None:
            x, y, z = template_coords[template_pos]

            # Ajouter du bruit pour diversification
            if noise_std > 0:
                x += np.random.normal(0, noise_std)
                y += np.random.normal(0, noise_std)
                z += np.random.normal(0, noise_std)

            predicted_coords[query_pos] = (x, y, z)

    # Interpoler les positions manquantes
    predicted_coords = interpolate_missing_coords(predicted_coords)

    return predicted_coords

def interpolate_missing_coords(coords):
    """
    Interpole les coordonn√©es manquantes (0, 0, 0) par interpolation lin√©aire.
    """
    n = len(coords)
    result = list(coords)

    # Trouver les positions avec des coordonn√©es valides
    valid_indices = [i for i, c in enumerate(coords) if c != (0.0, 0.0, 0.0)]

    if len(valid_indices) < 2:
        # Pas assez de points pour interpoler, utiliser des coordonn√©es par d√©faut
        for i in range(n):
            if result[i] == (0.0, 0.0, 0.0):
                # Position sur une h√©lice simple
                result[i] = (i * 5.9, 0.0, 0.0)
        return result

    # Interpoler entre les points valides
    for i in range(n):
        if result[i] == (0.0, 0.0, 0.0):
            # Trouver les voisins valides les plus proches
            prev_valid = None
            next_valid = None

            for vi in valid_indices:
                if vi < i:
                    prev_valid = vi
                elif vi > i and next_valid is None:
                    next_valid = vi
                    break

            if prev_valid is not None and next_valid is not None:
                # Interpolation lin√©aire
                t = (i - prev_valid) / (next_valid - prev_valid)
                x = coords[prev_valid][0] + t * (coords[next_valid][0] - coords[prev_valid][0])
                y = coords[prev_valid][1] + t * (coords[next_valid][1] - coords[prev_valid][1])
                z = coords[prev_valid][2] + t * (coords[next_valid][2] - coords[prev_valid][2])
                result[i] = (x, y, z)
            elif prev_valid is not None:
                # Extrapolation depuis le dernier point valide
                dx = 5.9  # Distance moyenne C1'-C1'
                result[i] = (coords[prev_valid][0] + dx * (i - prev_valid),
                           coords[prev_valid][1],
                           coords[prev_valid][2])
            elif next_valid is not None:
                # Extrapolation avant le premier point valide
                dx = 5.9
                result[i] = (coords[next_valid][0] - dx * (next_valid - i),
                           coords[next_valid][1],
                           coords[next_valid][2])

    return result

# Test de pr√©diction
print("üß¨ Test de pr√©diction de structure...")
if best_templates:
    pred_coords = predict_structure_from_template(test_seq, best_templates[0])
    valid_coords = sum(1 for c in pred_coords if c != (0.0, 0.0, 0.0))
    print(f"   Coordonn√©es pr√©dites: {len(pred_coords)} r√©sidus, {valid_coords} valides")
    print(f"   Premiers r√©sidus: {pred_coords[:3]}")

# %% [markdown]
# ### 10.5 G√©n√©ration de 5 pr√©dictions diversifi√©es

# %%
def generate_diverse_predictions(query_seq, templates_db, n_predictions=5):
    """
    G√©n√®re 5 pr√©dictions diversifi√©es pour une s√©quence.

    Strat√©gies de diversification :
    1. Utiliser les top-5 templates diff√©rents
    2. Ajouter du bruit aux coordonn√©es
    3. Combiner plusieurs templates
    """
    predictions = []

    # Trouver les meilleurs templates
    best_templates = find_best_templates(query_seq, templates_db, top_k=10)

    if not best_templates:
        # Fallback : pr√©diction lin√©aire
        for i in range(n_predictions):
            coords = [(j * 5.9 + np.random.normal(0, 0.5),
                      np.random.normal(0, 1),
                      np.random.normal(0, 1)) for j in range(len(query_seq))]
            predictions.append(coords)
        return predictions

    # Strat√©gie 1-3 : Utiliser les 3 meilleurs templates
    for i in range(min(3, len(best_templates))):
        noise = i * 0.2  # Augmenter le bruit progressivement
        coords = predict_structure_from_template(query_seq, best_templates[i], noise_std=noise)
        predictions.append(coords)

    # Strat√©gie 4 : Moyenne des 2 meilleurs templates + bruit
    if len(best_templates) >= 2:
        coords1 = predict_structure_from_template(query_seq, best_templates[0])
        coords2 = predict_structure_from_template(query_seq, best_templates[1])

        avg_coords = []
        for c1, c2 in zip(coords1, coords2):
            avg = ((c1[0] + c2[0]) / 2 + np.random.normal(0, 0.3),
                   (c1[1] + c2[1]) / 2 + np.random.normal(0, 0.3),
                   (c1[2] + c2[2]) / 2 + np.random.normal(0, 0.3))
            avg_coords.append(avg)
        predictions.append(avg_coords)
    else:
        predictions.append(predict_structure_from_template(query_seq, best_templates[0], noise_std=0.5))

    # Strat√©gie 5 : Meilleur template avec perturbation al√©atoire
    coords = predict_structure_from_template(query_seq, best_templates[0], noise_std=0.8)
    predictions.append(coords)

    # S'assurer qu'on a exactement 5 pr√©dictions
    while len(predictions) < n_predictions:
        predictions.append(predict_structure_from_template(query_seq, best_templates[0],
                                                          noise_std=np.random.uniform(0.3, 1.0)))

    return predictions[:n_predictions]

# Test
print("üéØ G√©n√©ration de 5 pr√©dictions diversifi√©es...")
test_predictions = generate_diverse_predictions(test_seq, templates_db, n_predictions=5)
print(f"   Nombre de pr√©dictions: {len(test_predictions)}")
for i, pred in enumerate(test_predictions):
    print(f"   Pr√©diction {i+1}: {len(pred)} r√©sidus")

# %% [markdown]
# ## üìä 11. √âvaluation locale (TM-score simplifi√©)

# %%
def calculate_tm_score_simple(pred_coords, ref_coords):
    """
    Calcule un TM-score simplifi√© entre les coordonn√©es pr√©dites et de r√©f√©rence.
    Note: Version simplifi√©e sans l'optimisation de rotation/translation.
    """
    n = len(ref_coords)
    if n == 0:
        return 0.0

    # Calculer d0
    if n >= 30:
        d0 = 0.6 * (n - 0.5) ** (1/3) - 2.5
    else:
        d0_values = {12: 0.3, 15: 0.4, 19: 0.5, 23: 0.6, 29: 0.7}
        d0 = 0.3
        for threshold, value in d0_values.items():
            if n >= threshold:
                d0 = value

    d0 = max(d0, 0.5)  # Minimum d0

    # Centrer les coordonn√©es
    pred_arr = np.array(pred_coords)
    ref_arr = np.array(ref_coords)

    pred_centered = pred_arr - np.mean(pred_arr, axis=0)
    ref_centered = ref_arr - np.mean(ref_arr, axis=0)

    # Calculer les distances
    distances = np.sqrt(np.sum((pred_centered - ref_centered) ** 2, axis=1))

    # Calculer le TM-score
    tm_score = np.sum(1 / (1 + (distances / d0) ** 2)) / n

    return tm_score

# √âvaluation sur le jeu de validation
print("üìä √âvaluation sur le jeu de validation...")

# Prendre quelques exemples de validation
n_eval = min(10, len(validation_sequences))
eval_scores = []

for idx in range(n_eval):
    row = validation_sequences.iloc[idx]
    target_id = row['target_id']
    query_seq = row['sequence']

    # Obtenir les coordonn√©es de r√©f√©rence
    ref_data = validation_labels[validation_labels['ID'].str.startswith(target_id + '_')]
    if len(ref_data) == 0:
        continue

    ref_data = ref_data.sort_values('resid')
    ref_coords = list(zip(ref_data['x_1'].fillna(0),
                          ref_data['y_1'].fillna(0),
                          ref_data['z_1'].fillna(0)))

    # G√©n√©rer les pr√©dictions
    predictions = generate_diverse_predictions(query_seq, templates_db)

    # Calculer le meilleur TM-score (best of 5)
    best_score = 0
    for pred in predictions:
        # Ajuster la longueur si n√©cessaire
        if len(pred) != len(ref_coords):
            min_len = min(len(pred), len(ref_coords))
            pred = pred[:min_len]
            ref = ref_coords[:min_len]
        else:
            ref = ref_coords

        score = calculate_tm_score_simple(pred, ref)
        best_score = max(best_score, score)

    eval_scores.append({'target_id': target_id, 'tm_score': best_score, 'length': len(query_seq)})
    print(f"   {target_id}: TM-score = {best_score:.4f} (len={len(query_seq)})")

if eval_scores:
    mean_score = np.mean([s['tm_score'] for s in eval_scores])
    print(f"\nüìà TM-score moyen (validation): {mean_score:.4f}")

# %% [markdown]
# ## üìù 12. G√©n√©ration du fichier de soumission

# %%
def generate_submission(test_sequences_df, templates_db, output_path='submission.csv'):
    """
    G√©n√®re le fichier de soumission au format Kaggle.
    """
    print("üìù G√©n√©ration du fichier de soumission...")

    rows = []

    for idx, row in test_sequences_df.iterrows():
        target_id = row['target_id']
        query_seq = row['sequence']

        # G√©n√©rer 5 pr√©dictions
        predictions = generate_diverse_predictions(query_seq, templates_db)

        # Cr√©er les lignes pour chaque r√©sidu
        for resid, nucleotide in enumerate(query_seq, start=1):
            row_data = {
                'ID': f"{target_id}_{resid}",
                'resname': nucleotide,
                'resid': resid
            }

            # Ajouter les coordonn√©es pour chaque pr√©diction
            for pred_idx, pred_coords in enumerate(predictions, start=1):
                if resid - 1 < len(pred_coords):
                    x, y, z = pred_coords[resid - 1]
                else:
                    x, y, z = 0.0, 0.0, 0.0

                # Clipper les coordonn√©es selon les r√®gles
                x = np.clip(x, -999.999, 9999.999)
                y = np.clip(y, -999.999, 9999.999)
                z = np.clip(z, -999.999, 9999.999)

                row_data[f'x_{pred_idx}'] = round(x, 3)
                row_data[f'y_{pred_idx}'] = round(y, 3)
                row_data[f'z_{pred_idx}'] = round(z, 3)

            rows.append(row_data)

        if idx % 10 == 0:
            print(f"   Progression: {idx + 1}/{len(test_sequences_df)}")

    # Cr√©er le DataFrame
    submission_df = pd.DataFrame(rows)

    # Ordonner les colonnes
    coord_cols = []
    for i in range(1, 6):
        coord_cols.extend([f'x_{i}', f'y_{i}', f'z_{i}'])

    column_order = ['ID', 'resname', 'resid'] + coord_cols
    submission_df = submission_df[column_order]

    # Sauvegarder
    submission_df.to_csv(output_path, index=False)
    print(f"‚úÖ Soumission g√©n√©r√©e: {output_path}")
    print(f"   Shape: {submission_df.shape}")

    return submission_df

# G√©n√©ration de la soumission (d√©commentez pour ex√©cuter)
# submission = generate_submission(test_sequences, templates_db, 'submission.csv')

# %% [markdown]
# ## üéØ 13. R√©sum√© de la strat√©gie

# %%
print("=" * 70)
print("               üéØ R√âSUM√â DE LA STRAT√âGIE TBM")
print("=" * 70)
print("""
üìã PIPELINE IMPL√âMENT√â:

1. EXTRACTION DES TEMPLATES
   ‚Ä¢ Parser les fichiers CIF du PDB_RNA
   ‚Ä¢ Extraire s√©quences + coordonn√©es C1'
   ‚Ä¢ {n_templates} templates disponibles

2. RECHERCHE DE TEMPLATES
   ‚Ä¢ Alignement de s√©quence simple
   ‚Ä¢ Score combin√©: similarit√© + ratio de longueur
   ‚Ä¢ S√©lection des top-K templates

3. PR√âDICTION DE STRUCTURE
   ‚Ä¢ Copie des coordonn√©es du template
   ‚Ä¢ Interpolation des positions manquantes
   ‚Ä¢ 5 strat√©gies de diversification

4. DIVERSIFICATION (5 pr√©dictions)
   ‚Ä¢ Pr√©diction 1-3: Top 3 templates
   ‚Ä¢ Pr√©diction 4: Moyenne des 2 meilleurs
   ‚Ä¢ Pr√©diction 5: Perturbation al√©atoire

üìà AM√âLIORATIONS POSSIBLES:
   ‚Ä¢ Alignement plus sophistiqu√© (Smith-Waterman)
   ‚Ä¢ Rotation/translation optimale (Kabsch algorithm)
   ‚Ä¢ Deep Learning pour les cibles sans template
   ‚Ä¢ Utilisation des MSA pour l'alignement

üöÄ Pour soumettre:
   1. D√©commenter la g√©n√©ration de soumission
   2. Utiliser tous les fichiers PDB (max_files=None)
   3. Ex√©cuter le notebook complet
""".format(n_templates=len(templates_db)))
print("=" * 70)
