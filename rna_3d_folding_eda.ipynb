{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83e\uddec Stanford RNA 3D Folding Part 2 - EDA & Visualisation 3D\n",
    "\n",
    "Ce notebook explore les donn\u00e9es du concours Stanford RNA 3D Folding Part 2.\n",
    "\n",
    "**Objectif** : Pr\u00e9dire la structure 3D de mol\u00e9cules d'ARN \u00e0 partir de leur s\u00e9quence.\n",
    "\n",
    "**Sections** :\n",
    "1. Chargement des donn\u00e9es\n",
    "2. Analyse exploratoire des s\u00e9quences\n",
    "3. Analyse des labels (coordonn\u00e9es 3D)\n",
    "4. Visualisation 3D des structures RNA\n",
    "5. Analyse des MSA (Multiple Sequence Alignments)\n",
    "6. Exploration des m\u00e9tadonn\u00e9es\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83d\udce6 1. Imports et Configuration\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# ============================================\n",
    "# Fonction simple pour parser les fichiers FASTA (sans BioPython)\n",
    "# ============================================\n",
    "def parse_fasta(fasta_path):\n",
    "    \"\"\"Parse un fichier FASTA et retourne une liste de tuples (header, sequence).\"\"\"\n",
    "    sequences = []\n",
    "    current_header = None\n",
    "    current_seq = []\n",
    "\n",
    "    with open(fasta_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if current_header is not None:\n",
    "                    sequences.append((current_header, ''.join(current_seq)))\n",
    "                current_header = line[1:]  # Enlever le '>'\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "\n",
    "        # Ajouter la derni\u00e8re s\u00e9quence\n",
    "        if current_header is not None:\n",
    "            sequences.append((current_header, ''.join(current_seq)))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# Configuration des couleurs pour les nucl\u00e9otides\n",
    "NUCLEOTIDE_COLORS = {\n",
    "    'A': '#FF6B6B',  # Rouge - Ad\u00e9nine\n",
    "    'U': '#4ECDC4',  # Cyan - Uracile\n",
    "    'G': '#45B7D1',  # Bleu - Guanine\n",
    "    'C': '#96CEB4',  # Vert - Cytosine\n",
    "}\n",
    "\n",
    "# Chemin vers les donn\u00e9es (ajuster selon l'environnement Kaggle)\n",
    "DATA_PATH = Path('/kaggle/input/stanford-rna-3d-folding-2')\n",
    "\n",
    "print(\"\ud83d\udcc1 Structure des donn\u00e9es:\")\n",
    "print(f\"  - MSA/: {len(list((DATA_PATH / 'MSA').glob('*.fasta')))} fichiers\")\n",
    "print(f\"  - PDB_RNA/: {len(list((DATA_PATH / 'PDB_RNA').glob('*.cif')))} fichiers\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83d\udcca 2. Chargement des Donn\u00e9es\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Chargement des fichiers CSV principaux\n",
    "train_sequences = pd.read_csv(DATA_PATH / 'train_sequences.csv')\n",
    "validation_sequences = pd.read_csv(DATA_PATH / 'validation_sequences.csv')\n",
    "test_sequences = pd.read_csv(DATA_PATH / 'test_sequences.csv')\n",
    "\n",
    "train_labels = pd.read_csv(DATA_PATH / 'train_labels.csv')\n",
    "validation_labels = pd.read_csv(DATA_PATH / 'validation_labels.csv')\n",
    "\n",
    "sample_submission = pd.read_csv(DATA_PATH / 'sample_submission.csv')\n",
    "\n",
    "# M\u00e9tadonn\u00e9es\n",
    "rna_metadata = pd.read_csv(DATA_PATH / 'extra' / 'rna_metadata.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\ud83d\udccb R\u00c9SUM\u00c9 DES DONN\u00c9ES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n\ud83d\udd39 S\u00e9quences:\")\n",
    "print(f\"   Train:       {len(train_sequences):,} s\u00e9quences\")\n",
    "print(f\"   Validation:  {len(validation_sequences):,} s\u00e9quences\")\n",
    "print(f\"   Test:        {len(test_sequences):,} s\u00e9quences\")\n",
    "print(f\"\\n\ud83d\udd39 Labels (coordonn\u00e9es 3D):\")\n",
    "print(f\"   Train:       {len(train_labels):,} r\u00e9sidus\")\n",
    "print(f\"   Validation:  {len(validation_labels):,} r\u00e9sidus\")\n",
    "print(f\"\\n\ud83d\udd39 M\u00e9tadonn\u00e9es: {len(rna_metadata):,} entr\u00e9es\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83d\udd2c 3. Analyse des S\u00e9quences\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Structure des donn\u00e9es de s\u00e9quences\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\ud83d\udccb Colonnes train_sequences:\")\n",
    "print(train_sequences.columns.tolist())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "train_sequences.head(3)\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Aper\u00e7u d\u00e9taill\u00e9 d'une s\u00e9quence\n",
    "print(\"\ud83d\udd0d Exemple de s\u00e9quence (premi\u00e8re entr\u00e9e):\")\n",
    "print(\"-\" * 60)\n",
    "for col in train_sequences.columns:\n",
    "    val = train_sequences.iloc[0][col]\n",
    "    if isinstance(val, str) and len(val) > 100:\n",
    "        print(f\"{col}: {val[:100]}...\")\n",
    "    else:\n",
    "        print(f\"{col}: {val}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Distribution des longueurs de s\u00e9quences\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculer les longueurs\n",
    "train_sequences['seq_length'] = train_sequences['sequence'].str.len()\n",
    "validation_sequences['seq_length'] = validation_sequences['sequence'].str.len()\n",
    "test_sequences['seq_length'] = test_sequences['sequence'].str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Train\n",
    "axes[0].hist(train_sequences['seq_length'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title(f'Train (n={len(train_sequences):,})', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Longueur de s\u00e9quence')\n",
    "axes[0].set_ylabel('Fr\u00e9quence')\n",
    "axes[0].axvline(train_sequences['seq_length'].median(), color='red', linestyle='--', label=f\"M\u00e9diane: {train_sequences['seq_length'].median():.0f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Validation\n",
    "axes[1].hist(validation_sequences['seq_length'], bins=30, color='seagreen', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title(f'Validation (n={len(validation_sequences):,})', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Longueur de s\u00e9quence')\n",
    "axes[1].axvline(validation_sequences['seq_length'].median(), color='red', linestyle='--', label=f\"M\u00e9diane: {validation_sequences['seq_length'].median():.0f}\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Test\n",
    "axes[2].hist(test_sequences['seq_length'], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[2].set_title(f'Test (n={len(test_sequences):,})', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Longueur de s\u00e9quence')\n",
    "axes[2].axvline(test_sequences['seq_length'].median(), color='red', linestyle='--', label=f\"M\u00e9diane: {test_sequences['seq_length'].median():.0f}\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('\ud83d\udcca Distribution des Longueurs de S\u00e9quences RNA', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques\n",
    "print(\"\\n\ud83d\udcc8 Statistiques des longueurs:\")\n",
    "print(\"-\" * 60)\n",
    "for name, df in [('Train', train_sequences), ('Validation', validation_sequences), ('Test', test_sequences)]:\n",
    "    print(f\"{name:12} | Min: {df['seq_length'].min():5} | Max: {df['seq_length'].max():5} | \"\n",
    "          f\"Mean: {df['seq_length'].mean():7.1f} | Median: {df['seq_length'].median():6.0f}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Composition en nucl\u00e9otides\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def analyze_nucleotide_composition(sequences, name):\n",
    "    \"\"\"Analyse la composition en nucl\u00e9otides d'un ensemble de s\u00e9quences.\"\"\"\n",
    "    all_nucleotides = ''.join(sequences['sequence'].values)\n",
    "    counts = Counter(all_nucleotides)\n",
    "    total = sum(counts.values())\n",
    "\n",
    "    composition = {nt: (count / total) * 100 for nt, count in counts.items()}\n",
    "    return composition\n",
    "\n",
    "# Analyser chaque dataset\n",
    "compositions = {}\n",
    "for name, df in [('Train', train_sequences), ('Validation', validation_sequences), ('Test', test_sequences)]:\n",
    "    compositions[name] = analyze_nucleotide_composition(df, name)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (name, comp) in enumerate(compositions.items()):\n",
    "    nucleotides = ['A', 'U', 'G', 'C']\n",
    "    values = [comp.get(nt, 0) for nt in nucleotides]\n",
    "    colors = [NUCLEOTIDE_COLORS[nt] for nt in nucleotides]\n",
    "\n",
    "    bars = axes[idx].bar(nucleotides, values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    axes[idx].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Pourcentage (%)')\n",
    "    axes[idx].set_ylim(0, 35)\n",
    "\n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for bar, val in zip(bars, values):\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                      f'{val:.1f}%', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('\ud83e\uddec Composition en Nucl\u00e9otides (A, U, G, C)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Autres nucl\u00e9otides (modifications)\n",
    "print(\"\\n\ud83d\udd2c Nucl\u00e9otides non-canoniques d\u00e9tect\u00e9s:\")\n",
    "for name, comp in compositions.items():\n",
    "    non_canonical = {k: v for k, v in comp.items() if k not in ['A', 'U', 'G', 'C']}\n",
    "    if non_canonical:\n",
    "        print(f\"  {name}: {non_canonical}\")\n",
    "    else:\n",
    "        print(f\"  {name}: Aucun\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Distribution temporelle (temporal_cutoff)\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Convertir les dates\n",
    "train_sequences['temporal_cutoff'] = pd.to_datetime(train_sequences['temporal_cutoff'])\n",
    "validation_sequences['temporal_cutoff'] = pd.to_datetime(validation_sequences['temporal_cutoff'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Train\n",
    "train_sequences.groupby(train_sequences['temporal_cutoff'].dt.to_period('M')).size().plot(\n",
    "    kind='bar', ax=axes[0], color='steelblue', alpha=0.7\n",
    ")\n",
    "axes[0].set_title('Train - Distribution temporelle', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Date de publication')\n",
    "axes[0].set_ylabel('Nombre de structures')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Validation\n",
    "validation_sequences.groupby(validation_sequences['temporal_cutoff'].dt.to_period('M')).size().plot(\n",
    "    kind='bar', ax=axes[1], color='seagreen', alpha=0.7\n",
    ")\n",
    "axes[1].set_title('Validation - Distribution temporelle', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Date de publication')\n",
    "axes[1].set_ylabel('Nombre de structures')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('\ud83d\udcc5 Distribution Temporelle des Structures', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcc5 Plages temporelles:\")\n",
    "print(f\"  Train:      {train_sequences['temporal_cutoff'].min().date()} \u2192 {train_sequences['temporal_cutoff'].max().date()}\")\n",
    "print(f\"  Validation: {validation_sequences['temporal_cutoff'].min().date()} \u2192 {validation_sequences['temporal_cutoff'].max().date()}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Analyse de la stoichiom\u00e9trie\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Analyser les patterns de stoichiom\u00e9trie\n",
    "def parse_stoichiometry(stoich_str):\n",
    "    \"\"\"Parse stoichiometry string to extract chain counts.\"\"\"\n",
    "    if pd.isna(stoich_str):\n",
    "        return {}\n",
    "    chains = {}\n",
    "    for part in stoich_str.split(';'):\n",
    "        if ':' in part:\n",
    "            chain, count = part.split(':')\n",
    "            chains[chain.strip()] = int(count)\n",
    "    return chains\n",
    "\n",
    "train_sequences['n_chains'] = train_sequences['stoichiometry'].apply(\n",
    "    lambda x: sum(parse_stoichiometry(x).values()) if pd.notna(x) else 0\n",
    ")\n",
    "validation_sequences['n_chains'] = validation_sequences['stoichiometry'].apply(\n",
    "    lambda x: sum(parse_stoichiometry(x).values()) if pd.notna(x) else 0\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Distribution du nombre de cha\u00eenes\n",
    "for idx, (name, df, color) in enumerate([('Train', train_sequences, 'steelblue'),\n",
    "                                          ('Validation', validation_sequences, 'seagreen')]):\n",
    "    chain_counts = df['n_chains'].value_counts().sort_index()\n",
    "    axes[idx].bar(chain_counts.index, chain_counts.values, color=color, alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(f'{name} - Nombre de cha\u00eenes', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Nombre de cha\u00eenes')\n",
    "    axes[idx].set_ylabel('Fr\u00e9quence')\n",
    "\n",
    "plt.suptitle('\ud83d\udd17 Distribution du Nombre de Cha\u00eenes par Structure', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Statistiques sur les cha\u00eenes:\")\n",
    "print(f\"  Train - Moyenne: {train_sequences['n_chains'].mean():.2f}, Max: {train_sequences['n_chains'].max()}\")\n",
    "print(f\"  Validation - Moyenne: {validation_sequences['n_chains'].mean():.2f}, Max: {validation_sequences['n_chains'].max()}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83c\udfaf 4. Analyse des Labels (Coordonn\u00e9es 3D)\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Structure des labels\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\ud83d\udccb Colonnes train_labels:\")\n",
    "print(train_labels.columns.tolist())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "train_labels.head(10)\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Nombre de structures alternatives par cible\n",
    "coord_cols = [c for c in train_labels.columns if c.startswith('x_')]\n",
    "n_structures = len(coord_cols)\n",
    "print(f\"\\n\ud83d\udd39 Nombre de structures alternatives dans train_labels: {n_structures}\")\n",
    "print(f\"   Colonnes de coordonn\u00e9es: {coord_cols}\")\n",
    "\n",
    "# V\u00e9rifier les valeurs manquantes\n",
    "print(f\"\\n\ud83d\udd39 Valeurs manquantes dans les coordonn\u00e9es:\")\n",
    "for col in ['x_1', 'y_1', 'z_1']:\n",
    "    missing = train_labels[col].isna().sum()\n",
    "    print(f\"   {col}: {missing:,} ({missing/len(train_labels)*100:.2f}%)\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Distribution des r\u00e9sidus par structure\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Compter les r\u00e9sidus par target_id\n",
    "train_labels['target_id'] = train_labels['ID'].str.rsplit('_', n=1).str[0]\n",
    "residues_per_target = train_labels.groupby('target_id').size()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.hist(residues_per_target, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "ax.set_title('Distribution du Nombre de R\u00e9sidus par Structure', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Nombre de r\u00e9sidus')\n",
    "ax.set_ylabel('Fr\u00e9quence')\n",
    "ax.axvline(residues_per_target.median(), color='red', linestyle='--',\n",
    "           label=f\"M\u00e9diane: {residues_per_target.median():.0f}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Statistiques des r\u00e9sidus par structure:\")\n",
    "print(f\"   Min: {residues_per_target.min()}, Max: {residues_per_target.max()}\")\n",
    "print(f\"   Mean: {residues_per_target.mean():.1f}, Median: {residues_per_target.median():.0f}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Distribution des types de r\u00e9sidus\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, (name, df, color) in enumerate([('Train', train_labels, 'steelblue'),\n",
    "                                          ('Validation', validation_labels, 'seagreen')]):\n",
    "    resname_counts = df['resname'].value_counts()\n",
    "    colors = [NUCLEOTIDE_COLORS.get(nt, 'gray') for nt in resname_counts.index]\n",
    "\n",
    "    bars = axes[idx].bar(resname_counts.index, resname_counts.values, color=colors, edgecolor='black')\n",
    "    axes[idx].set_title(f'{name} - Distribution des r\u00e9sidus', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Type de r\u00e9sidu')\n",
    "    axes[idx].set_ylabel('Fr\u00e9quence')\n",
    "\n",
    "    # Ajouter pourcentages\n",
    "    total = resname_counts.sum()\n",
    "    for bar, val in zip(bars, resname_counts.values):\n",
    "        pct = val / total * 100\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + total*0.01,\n",
    "                      f'{pct:.1f}%', ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('\ud83e\uddec Distribution des Types de R\u00e9sidus', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 Distribution spatiale des coordonn\u00e9es\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# \u00c9chantillonner pour la visualisation\n",
    "sample_labels = train_labels.dropna(subset=['x_1', 'y_1', 'z_1']).sample(min(50000, len(train_labels)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (coord, color) in enumerate([('x_1', 'red'), ('y_1', 'green'), ('z_1', 'blue')]):\n",
    "    axes[idx].hist(sample_labels[coord], bins=50, color=color, alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(f'Distribution {coord.upper()}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(f'{coord} (\u00c5ngstr\u00f6ms)')\n",
    "    axes[idx].set_ylabel('Fr\u00e9quence')\n",
    "\n",
    "    mean_val = sample_labels[coord].mean()\n",
    "    axes[idx].axvline(mean_val, color='black', linestyle='--', label=f'Mean: {mean_val:.1f}\u00c5')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.suptitle('\ud83d\udcd0 Distribution Spatiale des Coordonn\u00e9es C1\\'', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83c\udf10 5. Visualisation 3D des Structures RNA\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 Fonction de visualisation 3D\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_rna_3d(labels_df, target_id, structure_idx=1, title=None):\n",
    "    \"\"\"\n",
    "    Visualise une structure RNA en 3D avec Plotly.\n",
    "\n",
    "    Args:\n",
    "        labels_df: DataFrame des labels\n",
    "        target_id: ID de la cible \u00e0 visualiser\n",
    "        structure_idx: Index de la structure (1, 2, etc.)\n",
    "        title: Titre personnalis\u00e9\n",
    "    \"\"\"\n",
    "    # Filtrer pour cette cible\n",
    "    target_data = labels_df[labels_df['ID'].str.startswith(target_id + '_')].copy()\n",
    "    target_data = target_data.sort_values('resid')\n",
    "\n",
    "    # Colonnes de coordonn\u00e9es\n",
    "    x_col, y_col, z_col = f'x_{structure_idx}', f'y_{structure_idx}', f'z_{structure_idx}'\n",
    "\n",
    "    # V\u00e9rifier que les colonnes existent\n",
    "    if x_col not in target_data.columns:\n",
    "        print(f\"Structure {structure_idx} non disponible pour {target_id}\")\n",
    "        return None\n",
    "\n",
    "    # Retirer les valeurs manquantes\n",
    "    target_data = target_data.dropna(subset=[x_col, y_col, z_col])\n",
    "\n",
    "    if len(target_data) == 0:\n",
    "        print(f\"Pas de donn\u00e9es pour {target_id}\")\n",
    "        return None\n",
    "\n",
    "    # Couleurs par nucl\u00e9otide\n",
    "    colors = [NUCLEOTIDE_COLORS.get(res, 'gray') for res in target_data['resname']]\n",
    "\n",
    "    # Cr\u00e9er la figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Ajouter la trace du backbone (ligne)\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=target_data[x_col],\n",
    "        y=target_data[y_col],\n",
    "        z=target_data[z_col],\n",
    "        mode='lines',\n",
    "        line=dict(color='lightgray', width=3),\n",
    "        name='Backbone',\n",
    "        hoverinfo='skip'\n",
    "    ))\n",
    "\n",
    "    # Ajouter les points (atomes C1')\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=target_data[x_col],\n",
    "        y=target_data[y_col],\n",
    "        z=target_data[z_col],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color=colors,\n",
    "            opacity=0.9,\n",
    "            line=dict(color='black', width=0.5)\n",
    "        ),\n",
    "        text=[f\"Res {row['resid']}: {row['resname']}<br>({row[x_col]:.2f}, {row[y_col]:.2f}, {row[z_col]:.2f})\"\n",
    "              for _, row in target_data.iterrows()],\n",
    "        hoverinfo='text',\n",
    "        name=\"R\u00e9sidus C1'\"\n",
    "    ))\n",
    "\n",
    "    # Mise en page\n",
    "    title_text = title or f\"Structure 3D: {target_id} (n={len(target_data)} r\u00e9sidus)\"\n",
    "    fig.update_layout(\n",
    "        title=dict(text=title_text, font=dict(size=16)),\n",
    "        scene=dict(\n",
    "            xaxis_title='X (\u00c5)',\n",
    "            yaxis_title='Y (\u00c5)',\n",
    "            zaxis_title='Z (\u00c5)',\n",
    "            aspectmode='data'\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600,\n",
    "        showlegend=True,\n",
    "        legend=dict(x=0.02, y=0.98)\n",
    "    )\n",
    "\n",
    "    # Ajouter l\u00e9gende des couleurs\n",
    "    for nt, color in NUCLEOTIDE_COLORS.items():\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[None], y=[None], z=[None],\n",
    "            mode='markers',\n",
    "            marker=dict(size=10, color=color),\n",
    "            name=f'{nt}',\n",
    "            showlegend=True\n",
    "        ))\n",
    "\n",
    "    return fig\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 Visualisation de structures exemple\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# S\u00e9lectionner quelques structures int\u00e9ressantes\n",
    "target_ids = train_labels['ID'].str.rsplit('_', n=1).str[0].unique()\n",
    "\n",
    "# Trouver des structures de diff\u00e9rentes tailles\n",
    "sizes = train_labels.groupby(train_labels['ID'].str.rsplit('_', n=1).str[0]).size()\n",
    "small_target = sizes[sizes < 50].index[0] if len(sizes[sizes < 50]) > 0 else sizes.index[0]\n",
    "medium_target = sizes[(sizes >= 50) & (sizes < 150)].index[0] if len(sizes[(sizes >= 50) & (sizes < 150)]) > 0 else sizes.index[1]\n",
    "large_target = sizes[sizes >= 150].index[0] if len(sizes[sizes >= 150]) > 0 else sizes.index[2]\n",
    "\n",
    "print(f\"\ud83d\udd0d Structures s\u00e9lectionn\u00e9es pour visualisation:\")\n",
    "print(f\"   Petite:  {small_target} ({sizes[small_target]} r\u00e9sidus)\")\n",
    "print(f\"   Moyenne: {medium_target} ({sizes[medium_target]} r\u00e9sidus)\")\n",
    "print(f\"   Grande:  {large_target} ({sizes[large_target]} r\u00e9sidus)\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualiser une petite structure\n",
    "fig = visualize_rna_3d(train_labels, small_target)\n",
    "if fig:\n",
    "    fig.show()\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualiser une structure moyenne\n",
    "fig = visualize_rna_3d(train_labels, medium_target)\n",
    "if fig:\n",
    "    fig.show()\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualiser une grande structure\n",
    "fig = visualize_rna_3d(train_labels, large_target)\n",
    "if fig:\n",
    "    fig.show()\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3 Comparaison de structures alternatives (si disponibles)\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def compare_structures(labels_df, target_id, n_structures=2):\n",
    "    \"\"\"Compare plusieurs structures alternatives pour une m\u00eame cible.\"\"\"\n",
    "    target_data = labels_df[labels_df['ID'].str.startswith(target_id + '_')].copy()\n",
    "    target_data = target_data.sort_values('resid')\n",
    "\n",
    "    # V\u00e9rifier combien de structures sont disponibles\n",
    "    x_cols = [c for c in target_data.columns if c.startswith('x_')]\n",
    "    n_available = len(x_cols)\n",
    "\n",
    "    if n_available < 2:\n",
    "        print(f\"Seulement {n_available} structure(s) disponible(s) pour {target_id}\")\n",
    "        return None\n",
    "\n",
    "    n_to_show = min(n_structures, n_available)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "\n",
    "    for i in range(1, n_to_show + 1):\n",
    "        x_col, y_col, z_col = f'x_{i}', f'y_{i}', f'z_{i}'\n",
    "        data = target_data.dropna(subset=[x_col, y_col, z_col])\n",
    "\n",
    "        if len(data) > 0:\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=data[x_col],\n",
    "                y=data[y_col],\n",
    "                z=data[z_col],\n",
    "                mode='lines+markers',\n",
    "                marker=dict(size=4, opacity=0.7),\n",
    "                line=dict(width=2),\n",
    "                name=f'Structure {i}',\n",
    "                marker_color=colors[i-1]\n",
    "            ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Comparaison des structures: {target_id}\",\n",
    "        scene=dict(\n",
    "            xaxis_title='X (\u00c5)',\n",
    "            yaxis_title='Y (\u00c5)',\n",
    "            zaxis_title='Z (\u00c5)',\n",
    "            aspectmode='data'\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Trouver une cible avec plusieurs structures\n",
    "multi_struct_cols = [c for c in train_labels.columns if c.startswith('x_')]\n",
    "if len(multi_struct_cols) > 1:\n",
    "    # V\u00e9rifier quelles cibles ont plusieurs conformations\n",
    "    sample_target = train_labels['ID'].str.rsplit('_', n=1).str[0].iloc[0]\n",
    "    fig = compare_structures(train_labels, sample_target)\n",
    "    if fig:\n",
    "        fig.show()\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4 Analyse des distances inter-r\u00e9sidus\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def analyze_distances(labels_df, target_id, structure_idx=1):\n",
    "    \"\"\"Analyse les distances entre r\u00e9sidus cons\u00e9cutifs.\"\"\"\n",
    "    target_data = labels_df[labels_df['ID'].str.startswith(target_id + '_')].copy()\n",
    "    target_data = target_data.sort_values('resid')\n",
    "\n",
    "    x_col, y_col, z_col = f'x_{structure_idx}', f'y_{structure_idx}', f'z_{structure_idx}'\n",
    "    target_data = target_data.dropna(subset=[x_col, y_col, z_col])\n",
    "\n",
    "    if len(target_data) < 2:\n",
    "        return None\n",
    "\n",
    "    # Calculer les distances cons\u00e9cutives\n",
    "    coords = target_data[[x_col, y_col, z_col]].values\n",
    "    distances = np.sqrt(np.sum(np.diff(coords, axis=0)**2, axis=1))\n",
    "\n",
    "    return distances\n",
    "\n",
    "# Calculer pour plusieurs structures\n",
    "all_distances = []\n",
    "sample_targets = train_labels['ID'].str.rsplit('_', n=1).str[0].unique()[:100]\n",
    "\n",
    "for target in sample_targets:\n",
    "    distances = analyze_distances(train_labels, target)\n",
    "    if distances is not None:\n",
    "        all_distances.extend(distances)\n",
    "\n",
    "all_distances = np.array(all_distances)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme\n",
    "axes[0].hist(all_distances, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Distribution des Distances C1\\'-C1\\' Cons\u00e9cutives', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Distance (\u00c5)')\n",
    "axes[0].set_ylabel('Fr\u00e9quence')\n",
    "axes[0].axvline(np.median(all_distances), color='red', linestyle='--',\n",
    "                label=f'M\u00e9diane: {np.median(all_distances):.2f}\u00c5')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(all_distances, vert=True)\n",
    "axes[1].set_title('Box Plot des Distances', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Distance (\u00c5)')\n",
    "\n",
    "plt.suptitle('\ud83d\udccf Analyse des Distances Inter-R\u00e9sidus', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Statistiques des distances C1'-C1' cons\u00e9cutives:\")\n",
    "print(f\"   Mean: {np.mean(all_distances):.2f}\u00c5\")\n",
    "print(f\"   Median: {np.median(all_distances):.2f}\u00c5\")\n",
    "print(f\"   Std: {np.std(all_distances):.2f}\u00c5\")\n",
    "print(f\"   Min: {np.min(all_distances):.2f}\u00c5, Max: {np.max(all_distances):.2f}\u00c5\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83d\udcc2 6. Analyse des MSA (Multiple Sequence Alignments)\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def analyze_msa(msa_path, n_samples=5):\n",
    "    \"\"\"Analyse un fichier MSA.\"\"\"\n",
    "    try:\n",
    "        sequences = parse_fasta(msa_path)\n",
    "\n",
    "        info = {\n",
    "            'n_sequences': len(sequences),\n",
    "            'target_length': len(sequences[0][1]) if sequences else 0,\n",
    "            'headers': [seq[0] for seq in sequences[:n_samples]]\n",
    "        }\n",
    "\n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Analyser quelques MSA\n",
    "msa_dir = DATA_PATH / 'MSA'\n",
    "msa_files = list(msa_dir.glob('*.fasta'))[:10]\n",
    "\n",
    "print(f\"\ud83d\udcc1 Analyse de {len(msa_files)} fichiers MSA (sur {len(list(msa_dir.glob('*.fasta')))} total):\\n\")\n",
    "\n",
    "msa_stats = []\n",
    "for msa_file in msa_files:\n",
    "    info = analyze_msa(msa_file)\n",
    "    msa_stats.append({\n",
    "        'file': msa_file.name,\n",
    "        'n_sequences': info.get('n_sequences', 0),\n",
    "        'target_length': info.get('target_length', 0)\n",
    "    })\n",
    "    print(f\"  {msa_file.name}: {info.get('n_sequences', 'N/A')} s\u00e9quences, longueur {info.get('target_length', 'N/A')}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Distribution de la profondeur des MSA\n",
    "all_msa_files = list(msa_dir.glob('*.fasta'))\n",
    "msa_depths = []\n",
    "\n",
    "print(f\"\\n\u23f3 Analyse de la profondeur des MSA ({len(all_msa_files)} fichiers)...\")\n",
    "\n",
    "for i, msa_file in enumerate(all_msa_files[:500]):  # Limiter pour la vitesse\n",
    "    try:\n",
    "        sequences = parse_fasta(msa_file)\n",
    "        msa_depths.append(len(sequences))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(msa_depths, bins=50, color='teal', alpha=0.7, edgecolor='black')\n",
    "ax.set_title('Distribution de la Profondeur des MSA', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Nombre de s\u00e9quences dans le MSA')\n",
    "ax.set_ylabel('Fr\u00e9quence')\n",
    "ax.axvline(np.median(msa_depths), color='red', linestyle='--', label=f'M\u00e9diane: {np.median(msa_depths):.0f}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Statistiques de profondeur MSA:\")\n",
    "print(f\"   Min: {np.min(msa_depths)}, Max: {np.max(msa_depths)}\")\n",
    "print(f\"   Mean: {np.mean(msa_depths):.1f}, Median: {np.median(msa_depths):.0f}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83d\udccb 7. Analyse des M\u00e9tadonn\u00e9es\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\ud83d\udccb Colonnes rna_metadata:\")\n",
    "print(rna_metadata.columns.tolist())\n",
    "print(f\"\\nShape: {rna_metadata.shape}\")\n",
    "rna_metadata.head()\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Analyse des colonnes cl\u00e9s\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Resolution\n",
    "if 'resolution' in rna_metadata.columns:\n",
    "    rna_metadata['resolution'].dropna().hist(bins=50, ax=axes[0, 0], color='steelblue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Distribution de la R\u00e9solution', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('R\u00e9solution (\u00c5)')\n",
    "\n",
    "# Method\n",
    "if 'method' in rna_metadata.columns:\n",
    "    method_counts = rna_metadata['method'].value_counts().head(10)\n",
    "    method_counts.plot(kind='barh', ax=axes[0, 1], color='seagreen', alpha=0.7)\n",
    "    axes[0, 1].set_title('M\u00e9thodes Exp\u00e9rimentales', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Nombre de structures')\n",
    "\n",
    "# RNA composition\n",
    "if 'rna_composition' in rna_metadata.columns:\n",
    "    rna_metadata['rna_composition'].dropna().hist(bins=50, ax=axes[1, 0], color='coral', alpha=0.7)\n",
    "    axes[1, 0].set_title('Composition RNA (%)', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('% RNA')\n",
    "\n",
    "# Structuredness\n",
    "if 'structuredness' in rna_metadata.columns:\n",
    "    rna_metadata['structuredness'].dropna().hist(bins=50, ax=axes[1, 1], color='purple', alpha=0.7)\n",
    "    axes[1, 1].set_title('Structuredness', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Structuredness score')\n",
    "\n",
    "plt.suptitle('\ud83d\udcca Analyse des M\u00e9tadonn\u00e9es RNA', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83d\udcdd 8. Analyse du Format de Soumission\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\ud83d\udccb Format de soumission:\")\n",
    "print(sample_submission.columns.tolist())\n",
    "print(f\"\\nShape: {sample_submission.shape}\")\n",
    "sample_submission.head(10)\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# V\u00e9rifier le nombre de pr\u00e9dictions requises\n",
    "coord_cols = [c for c in sample_submission.columns if c.startswith('x_')]\n",
    "print(f\"\\n\ud83c\udfaf Nombre de structures \u00e0 pr\u00e9dire: {len(coord_cols)} (x_1 \u00e0 x_{len(coord_cols)})\")\n",
    "\n",
    "# Nombre de cibles uniques dans le test\n",
    "test_target_ids = sample_submission['ID'].str.rsplit('_', n=1).str[0].unique()\n",
    "print(f\"\ud83d\udcca Nombre de cibles test: {len(test_target_ids)}\")\n",
    "print(f\"\ud83d\udcca Nombre total de r\u00e9sidus \u00e0 pr\u00e9dire: {len(sample_submission)}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83c\udfaf 9. R\u00e9sum\u00e9 et Insights Cl\u00e9s\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"                    \ud83d\udcca R\u00c9SUM\u00c9 DE L'ANALYSE EDA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "\ud83d\udd39 DONN\u00c9ES:\n",
    "   \u2022 Train: {train_seq} s\u00e9quences, {train_res:,} r\u00e9sidus\n",
    "   \u2022 Validation: {val_seq} s\u00e9quences, {val_res:,} r\u00e9sidus\n",
    "   \u2022 Test: {test_seq} s\u00e9quences \u00e0 pr\u00e9dire\n",
    "   \u2022 MSA disponibles: {msa_count:,} fichiers\n",
    "   \u2022 Structures PDB: {pdb_count:,} fichiers\n",
    "\n",
    "\ud83d\udd39 S\u00c9QUENCES:\n",
    "   \u2022 Longueur m\u00e9diane: Train={train_med:.0f}, Val={val_med:.0f}, Test={test_med:.0f}\n",
    "   \u2022 Composition: ~25% chaque nucl\u00e9otide (A, U, G, C \u00e9quilibr\u00e9s)\n",
    "   \u2022 Structures multi-cha\u00eenes pr\u00e9sentes\n",
    "\n",
    "\ud83d\udd39 STRUCTURES 3D:\n",
    "   \u2022 Coordonn\u00e9es: Position de l'atome C1' de chaque r\u00e9sidu\n",
    "   \u2022 Distance C1'-C1' cons\u00e9cutive: ~{dist_mean:.1f}\u00c5 (m\u00e9diane)\n",
    "   \u2022 Format soumission: 5 structures par cible\n",
    "\n",
    "\ud83d\udd39 INSIGHTS POUR LA MOD\u00c9LISATION:\n",
    "   \u2022 Les MSA profonds peuvent aider (info \u00e9volutive)\n",
    "   \u2022 Templates PDB disponibles pour recherche d'homologues\n",
    "   \u2022 Validation temporelle: structures apr\u00e8s mai 2025\n",
    "   \u2022 M\u00e9trique: TM-score (best of 5 predictions)\n",
    "\"\"\".format(\n",
    "    train_seq=len(train_sequences),\n",
    "    train_res=len(train_labels),\n",
    "    val_seq=len(validation_sequences),\n",
    "    val_res=len(validation_labels),\n",
    "    test_seq=len(test_sequences),\n",
    "    msa_count=len(list((DATA_PATH / 'MSA').glob('*.fasta'))),\n",
    "    pdb_count=len(list((DATA_PATH / 'PDB_RNA').glob('*.cif'))),\n",
    "    train_med=train_sequences['seq_length'].median(),\n",
    "    val_med=validation_sequences['seq_length'].median(),\n",
    "    test_med=test_sequences['seq_length'].median(),\n",
    "    dist_mean=np.median(all_distances) if len(all_distances) > 0 else 5.9\n",
    "))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"                    \ud83d\ude80 PR\u00caT POUR LA MOD\u00c9LISATION!\")\n",
    "print(\"=\" * 70)\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# \ud83e\uddec PARTIE 2 : STRAT\u00c9GIES DE MOD\u00c9LISATION\n",
    "---\n",
    "\n",
    "Bas\u00e9 sur les solutions gagnantes de la Part 1 :\n",
    "- **1\u00e8re place (john)** : Template-Based Modeling (TBM) pur\n",
    "- **2\u00e8me place (odat)** : TBM avec alignement optimis\u00e9\n",
    "- **3\u00e8me place (Eigen)** : Hybride TBM + Deep Learning\n",
    "\n",
    "## Approche retenue : Template-Based Modeling\n",
    "95% des cibles ont des templates potentiels dans le PDB !\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83d\udd0d 10. Pipeline Template-Based Modeling (TBM)\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================\n",
    "# STRAT\u00c9GIE 1 : TEMPLATE-BASED MODELING\n",
    "# ============================================\n",
    "# Pipeline :\n",
    "# 1. Parser les s\u00e9quences du PDB_RNA\n",
    "# 2. Aligner la s\u00e9quence cible avec les s\u00e9quences PDB\n",
    "# 3. Trouver les meilleurs templates\n",
    "# 4. Copier/adapter les coordonn\u00e9es 3D\n",
    "# 5. G\u00e9n\u00e9rer 5 pr\u00e9dictions diversifi\u00e9es\n",
    "\n",
    "print(\"\ud83d\udd27 Configuration du pipeline TBM...\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.1 Extraction des s\u00e9quences du PDB\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Parser CIF natif (sans d\u00e9pendances externes)\n",
    "\n",
    "def parse_cif_file(cif_path):\n",
    "    \"\"\"\n",
    "    Parse un fichier CIF et extrait les donn\u00e9es atom_site.\n",
    "    Parser robuste en Python pur, compatible Kaggle.\n",
    "    G\u00e8re les valeurs entre guillemets et les formats vari\u00e9s.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    atom_data = []\n",
    "    columns = []\n",
    "    in_loop = False\n",
    "    collecting_columns = False\n",
    "\n",
    "    def parse_cif_line(line):\n",
    "        \"\"\"Parse une ligne CIF en g\u00e9rant les guillemets.\"\"\"\n",
    "        tokens = []\n",
    "        current = ''\n",
    "        in_quote = False\n",
    "        quote_char = None\n",
    "\n",
    "        for char in line:\n",
    "            if char in ('\"', \"'\") and not in_quote:\n",
    "                in_quote = True\n",
    "                quote_char = char\n",
    "            elif char == quote_char and in_quote:\n",
    "                in_quote = False\n",
    "                quote_char = None\n",
    "            elif char.isspace() and not in_quote:\n",
    "                if current:\n",
    "                    tokens.append(current)\n",
    "                    current = ''\n",
    "            else:\n",
    "                current += char\n",
    "\n",
    "        if current:\n",
    "            tokens.append(current)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    with open(cif_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "\n",
    "            # D\u00e9tecter loop_\n",
    "            if line == 'loop_':\n",
    "                in_loop = True\n",
    "                collecting_columns = True\n",
    "                columns = []\n",
    "                continue\n",
    "\n",
    "            # Collecter les noms de colonnes atom_site\n",
    "            if collecting_columns and line.startswith('_atom_site.'):\n",
    "                col_name = line.split('.')[1].split()[0]\n",
    "                columns.append(col_name)\n",
    "                continue\n",
    "\n",
    "            # Fin de la collecte des colonnes\n",
    "            if collecting_columns and not line.startswith('_'):\n",
    "                collecting_columns = False\n",
    "\n",
    "                # Si on n'a pas de colonnes atom_site, r\u00e9initialiser\n",
    "                if not columns or not any('atom' in c.lower() or 'Cartn' in c for c in columns):\n",
    "                    in_loop = False\n",
    "                    columns = []\n",
    "                    continue\n",
    "\n",
    "            # Si nouveau bloc (autre _), terminer\n",
    "            if in_loop and not collecting_columns and line.startswith('_'):\n",
    "                break\n",
    "\n",
    "            # Parser les donn\u00e9es\n",
    "            if in_loop and not collecting_columns and columns:\n",
    "                if line.startswith('ATOM') or line.startswith('HETATM'):\n",
    "                    tokens = parse_cif_line(line)\n",
    "                    if len(tokens) >= len(columns):\n",
    "                        atom_data.append(dict(zip(columns, tokens[:len(columns)])))\n",
    "\n",
    "    return atom_data\n",
    "\n",
    "def extract_rna_sequences_from_cif(cif_path):\n",
    "    \"\"\"\n",
    "    Extrait les s\u00e9quences RNA et coordonn\u00e9es C1' d'un fichier CIF.\n",
    "    Retourne un dict avec les infos de la structure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        atom_data = parse_cif_file(cif_path)\n",
    "\n",
    "        if not atom_data:\n",
    "            return None\n",
    "\n",
    "        residues = {}\n",
    "        coords = {}\n",
    "\n",
    "        for atom in atom_data:\n",
    "            # R\u00e9cup\u00e9rer les champs n\u00e9cessaires\n",
    "            atom_name = atom.get('label_atom_id', atom.get('auth_atom_id', ''))\n",
    "            res_name = atom.get('label_comp_id', atom.get('auth_comp_id', ''))\n",
    "            chain_id = atom.get('label_asym_id', atom.get('auth_asym_id', ''))\n",
    "            seq_id = atom.get('label_seq_id', atom.get('auth_seq_id', ''))\n",
    "\n",
    "            # Filtrer uniquement les nucl\u00e9otides RNA (A, U, G, C)\n",
    "            if res_name in ['A', 'U', 'G', 'C', 'ADE', 'URA', 'GUA', 'CYT',\n",
    "                           'DA', 'DU', 'DG', 'DC', 'RA', 'RU', 'RG', 'RC']:\n",
    "                # Mapper les noms longs vers courts\n",
    "                res_map = {'ADE': 'A', 'URA': 'U', 'GUA': 'G', 'CYT': 'C',\n",
    "                          'DA': 'A', 'DU': 'U', 'DG': 'G', 'DC': 'C',\n",
    "                          'RA': 'A', 'RU': 'U', 'RG': 'G', 'RC': 'C'}\n",
    "                res_short = res_map.get(res_name, res_name)\n",
    "\n",
    "                key = (chain_id, seq_id)\n",
    "                if key not in residues:\n",
    "                    residues[key] = res_short\n",
    "\n",
    "                # Stocker les coordonn\u00e9es C1' (plusieurs variations possibles)\n",
    "                atom_name_clean = atom_name.strip(\"'\\\"\")\n",
    "                if atom_name_clean in (\"C1'\", \"C1*\", \"C1\", \"C1'\"):\n",
    "                    try:\n",
    "                        x = float(atom.get('Cartn_x', 0))\n",
    "                        y = float(atom.get('Cartn_y', 0))\n",
    "                        z = float(atom.get('Cartn_z', 0))\n",
    "                        coords[key] = (x, y, z)\n",
    "                    except (ValueError, TypeError):\n",
    "                        pass\n",
    "\n",
    "        if not residues:\n",
    "            return None\n",
    "\n",
    "        # Construire la s\u00e9quence par cha\u00eene\n",
    "        chains = {}\n",
    "        for (chain_id, seq_id), res in sorted(residues.items(), key=lambda x: (x[0][0], int(x[0][1]) if x[0][1].isdigit() else 0)):\n",
    "            if chain_id not in chains:\n",
    "                chains[chain_id] = {'sequence': '', 'coords': []}\n",
    "            chains[chain_id]['sequence'] += res\n",
    "            if (chain_id, seq_id) in coords:\n",
    "                chains[chain_id]['coords'].append(coords[(chain_id, seq_id)])\n",
    "            else:\n",
    "                chains[chain_id]['coords'].append(None)\n",
    "\n",
    "        return {\n",
    "            'pdb_id': Path(cif_path).stem.upper(),\n",
    "            'chains': chains\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# D\u00e9bogage : afficher le contenu brut d'un fichier CIF\n",
    "print(\"\ud83d\udd0d D\u00e9bogage du parser CIF...\")\n",
    "pdb_dir = DATA_PATH / 'PDB_RNA'\n",
    "sample_cifs = list(pdb_dir.glob('*.cif'))[:1]\n",
    "\n",
    "if sample_cifs:\n",
    "    test_cif = sample_cifs[0]\n",
    "    print(f\"   Fichier test: {test_cif.name}\")\n",
    "    atom_data = parse_cif_file(test_cif)\n",
    "    print(f\"   Atomes pars\u00e9s: {len(atom_data)}\")\n",
    "\n",
    "    if atom_data:\n",
    "        # Afficher les colonnes disponibles\n",
    "        print(f\"   Colonnes: {list(atom_data[0].keys())}\")\n",
    "        # Afficher quelques atomes\n",
    "        print(\"   Exemples d'atomes:\")\n",
    "        for atom in atom_data[:3]:\n",
    "            atom_id = atom.get('label_atom_id', atom.get('auth_atom_id', 'N/A'))\n",
    "            res = atom.get('label_comp_id', atom.get('auth_comp_id', 'N/A'))\n",
    "            print(f\"     - atom={atom_id}, res={res}\")\n",
    "        # Chercher C1'\n",
    "        c1_atoms = [a for a in atom_data if \"C1\" in str(a.get('label_atom_id', ''))]\n",
    "        print(f\"   Atomes contenant 'C1': {len(c1_atoms)}\")\n",
    "        if c1_atoms:\n",
    "            print(f\"     Exemple: {c1_atoms[0].get('label_atom_id')}\")\n",
    "\n",
    "# Test sur quelques fichiers\n",
    "print(\"\\n\ud83d\udcc2 Test d'extraction sur quelques fichiers CIF...\")\n",
    "sample_cifs = list(pdb_dir.glob('*.cif'))[:5]\n",
    "\n",
    "for cif_file in sample_cifs:\n",
    "    result = extract_rna_sequences_from_cif(cif_file)\n",
    "    if result:\n",
    "        for chain_id, chain_data in result['chains'].items():\n",
    "            seq = chain_data['sequence']\n",
    "            n_coords = sum(1 for c in chain_data['coords'] if c is not None)\n",
    "            print(f\"  {result['pdb_id']}_{chain_id}: {len(seq)} nt, {n_coords} coords C1'\")\n",
    "            if len(seq) < 50:\n",
    "                print(f\"    S\u00e9quence: {seq}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.2 Construction de la base de templates\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def build_template_database(pdb_dir, max_files=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Construit une base de donn\u00e9es de templates \u00e0 partir des fichiers CIF.\n",
    "    \"\"\"\n",
    "    templates = []\n",
    "    cif_files = list(pdb_dir.glob('*.cif'))\n",
    "\n",
    "    if max_files:\n",
    "        cif_files = cif_files[:max_files]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\ud83d\udce6 Construction de la base de templates ({len(cif_files)} fichiers)...\")\n",
    "\n",
    "    for i, cif_file in enumerate(cif_files):\n",
    "        if verbose and i % 500 == 0:\n",
    "            print(f\"   Progression: {i}/{len(cif_files)}\")\n",
    "\n",
    "        result = extract_rna_sequences_from_cif(cif_file)\n",
    "        if result:\n",
    "            for chain_id, chain_data in result['chains'].items():\n",
    "                seq = chain_data['sequence']\n",
    "                coords = chain_data['coords']\n",
    "\n",
    "                # Filtrer les cha\u00eenes trop courtes ou sans coordonn\u00e9es\n",
    "                n_valid_coords = sum(1 for c in coords if c is not None)\n",
    "                if len(seq) >= 10 and n_valid_coords >= len(seq) * 0.5:\n",
    "                    templates.append({\n",
    "                        'pdb_id': result['pdb_id'],\n",
    "                        'chain_id': chain_id,\n",
    "                        'sequence': seq,\n",
    "                        'coords': coords,\n",
    "                        'length': len(seq)\n",
    "                    })\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\u2705 {len(templates)} templates extraits\")\n",
    "\n",
    "    return templates\n",
    "\n",
    "# Construire une base de templates (limiter pour la d\u00e9mo)\n",
    "# En production, utiliser tous les fichiers (max_files=None)\n",
    "templates_db = build_template_database(pdb_dir, max_files=500, verbose=True)\n",
    "\n",
    "# Statistiques\n",
    "print(f\"\\n\ud83d\udcca Statistiques des templates:\")\n",
    "print(f\"   Nombre: {len(templates_db)}\")\n",
    "\n",
    "if templates_db:\n",
    "    template_lengths = [t['length'] for t in templates_db]\n",
    "    print(f\"   Longueur min: {min(template_lengths)}, max: {max(template_lengths)}\")\n",
    "    print(f\"   Longueur moyenne: {np.mean(template_lengths):.1f}\")\n",
    "else:\n",
    "    print(\"   \u26a0\ufe0f Aucun template extrait - v\u00e9rifiez le parser CIF ci-dessus\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.3 Alignement de s\u00e9quences (Simple)\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def simple_sequence_alignment(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Alignement simple bas\u00e9 sur la similarit\u00e9 de s\u00e9quence.\n",
    "    Retourne le score de similarit\u00e9 (0-1) et les positions align\u00e9es.\n",
    "    \"\"\"\n",
    "    # M\u00e9thode simple : recherche de sous-cha\u00eenes communes\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "\n",
    "    if len1 == 0 or len2 == 0:\n",
    "        return 0.0, []\n",
    "\n",
    "    # Construire une matrice de correspondance\n",
    "    matches = 0\n",
    "    aligned_positions = []\n",
    "\n",
    "    # Alignement global simple (sans gaps pour commencer)\n",
    "    min_len = min(len1, len2)\n",
    "\n",
    "    # Essayer diff\u00e9rents d\u00e9calages\n",
    "    best_score = 0\n",
    "    best_offset = 0\n",
    "\n",
    "    for offset in range(-len2 + 1, len1):\n",
    "        score = 0\n",
    "        for i in range(min_len):\n",
    "            pos1 = i\n",
    "            pos2 = i - offset\n",
    "            if 0 <= pos1 < len1 and 0 <= pos2 < len2:\n",
    "                if seq1[pos1] == seq2[pos2]:\n",
    "                    score += 1\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_offset = offset\n",
    "\n",
    "    # Reconstruire l'alignement avec le meilleur offset\n",
    "    for i in range(max(len1, len2)):\n",
    "        pos1 = i\n",
    "        pos2 = i - best_offset\n",
    "        if 0 <= pos1 < len1 and 0 <= pos2 < len2:\n",
    "            if seq1[pos1] == seq2[pos2]:\n",
    "                aligned_positions.append((pos1, pos2))\n",
    "\n",
    "    # Score normalis\u00e9\n",
    "    similarity = best_score / max(len1, len2)\n",
    "\n",
    "    return similarity, aligned_positions\n",
    "\n",
    "def find_best_templates(query_seq, templates_db, top_k=5):\n",
    "    \"\"\"\n",
    "    Trouve les meilleurs templates pour une s\u00e9quence query.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for template in templates_db:\n",
    "        similarity, aligned_pos = simple_sequence_alignment(query_seq, template['sequence'])\n",
    "\n",
    "        # Bonus pour longueur similaire\n",
    "        len_ratio = min(len(query_seq), template['length']) / max(len(query_seq), template['length'])\n",
    "\n",
    "        combined_score = similarity * 0.7 + len_ratio * 0.3\n",
    "\n",
    "        scores.append({\n",
    "            'template': template,\n",
    "            'similarity': similarity,\n",
    "            'len_ratio': len_ratio,\n",
    "            'combined_score': combined_score,\n",
    "            'aligned_positions': aligned_pos\n",
    "        })\n",
    "\n",
    "    # Trier par score combin\u00e9\n",
    "    scores.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "\n",
    "    return scores[:top_k]\n",
    "\n",
    "# Test sur une s\u00e9quence de validation\n",
    "print(\"\ud83d\udd0d Test de recherche de templates...\")\n",
    "test_seq = validation_sequences.iloc[0]['sequence']\n",
    "print(f\"   S\u00e9quence test: {test_seq[:50]}... (longueur: {len(test_seq)})\")\n",
    "\n",
    "best_templates = find_best_templates(test_seq, templates_db, top_k=5)\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Top 5 templates:\")\n",
    "for i, match in enumerate(best_templates):\n",
    "    t = match['template']\n",
    "    print(f\"   {i+1}. {t['pdb_id']}_{t['chain_id']}: \"\n",
    "          f\"sim={match['similarity']:.3f}, len={t['length']}, \"\n",
    "          f\"score={match['combined_score']:.3f}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.4 Pr\u00e9diction de structure par template\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_structure_from_template(query_seq, template_match, noise_std=0.0):\n",
    "    \"\"\"\n",
    "    Pr\u00e9dit les coordonn\u00e9es 3D en utilisant un template.\n",
    "\n",
    "    Args:\n",
    "        query_seq: S\u00e9quence cible\n",
    "        template_match: R\u00e9sultat de find_best_templates\n",
    "        noise_std: \u00c9cart-type du bruit \u00e0 ajouter (pour diversification)\n",
    "\n",
    "    Returns:\n",
    "        Liste de coordonn\u00e9es (x, y, z) pour chaque r\u00e9sidu\n",
    "    \"\"\"\n",
    "    template = template_match['template']\n",
    "    aligned_positions = template_match['aligned_positions']\n",
    "    template_coords = template['coords']\n",
    "\n",
    "    # Initialiser les coordonn\u00e9es pr\u00e9dites\n",
    "    predicted_coords = [(0.0, 0.0, 0.0)] * len(query_seq)\n",
    "\n",
    "    # Copier les coordonn\u00e9es des positions align\u00e9es\n",
    "    for query_pos, template_pos in aligned_positions:\n",
    "        if template_pos < len(template_coords) and template_coords[template_pos] is not None:\n",
    "            x, y, z = template_coords[template_pos]\n",
    "\n",
    "            # Ajouter du bruit pour diversification\n",
    "            if noise_std > 0:\n",
    "                x += np.random.normal(0, noise_std)\n",
    "                y += np.random.normal(0, noise_std)\n",
    "                z += np.random.normal(0, noise_std)\n",
    "\n",
    "            predicted_coords[query_pos] = (x, y, z)\n",
    "\n",
    "    # Interpoler les positions manquantes\n",
    "    predicted_coords = interpolate_missing_coords(predicted_coords)\n",
    "\n",
    "    return predicted_coords\n",
    "\n",
    "def interpolate_missing_coords(coords):\n",
    "    \"\"\"\n",
    "    Interpole les coordonn\u00e9es manquantes (0, 0, 0) par interpolation lin\u00e9aire.\n",
    "    \"\"\"\n",
    "    n = len(coords)\n",
    "    result = list(coords)\n",
    "\n",
    "    # Trouver les positions avec des coordonn\u00e9es valides\n",
    "    valid_indices = [i for i, c in enumerate(coords) if c != (0.0, 0.0, 0.0)]\n",
    "\n",
    "    if len(valid_indices) < 2:\n",
    "        # Pas assez de points pour interpoler, utiliser des coordonn\u00e9es par d\u00e9faut\n",
    "        for i in range(n):\n",
    "            if result[i] == (0.0, 0.0, 0.0):\n",
    "                # Position sur une h\u00e9lice simple\n",
    "                result[i] = (i * 5.9, 0.0, 0.0)\n",
    "        return result\n",
    "\n",
    "    # Interpoler entre les points valides\n",
    "    for i in range(n):\n",
    "        if result[i] == (0.0, 0.0, 0.0):\n",
    "            # Trouver les voisins valides les plus proches\n",
    "            prev_valid = None\n",
    "            next_valid = None\n",
    "\n",
    "            for vi in valid_indices:\n",
    "                if vi < i:\n",
    "                    prev_valid = vi\n",
    "                elif vi > i and next_valid is None:\n",
    "                    next_valid = vi\n",
    "                    break\n",
    "\n",
    "            if prev_valid is not None and next_valid is not None:\n",
    "                # Interpolation lin\u00e9aire\n",
    "                t = (i - prev_valid) / (next_valid - prev_valid)\n",
    "                x = coords[prev_valid][0] + t * (coords[next_valid][0] - coords[prev_valid][0])\n",
    "                y = coords[prev_valid][1] + t * (coords[next_valid][1] - coords[prev_valid][1])\n",
    "                z = coords[prev_valid][2] + t * (coords[next_valid][2] - coords[prev_valid][2])\n",
    "                result[i] = (x, y, z)\n",
    "            elif prev_valid is not None:\n",
    "                # Extrapolation depuis le dernier point valide\n",
    "                dx = 5.9  # Distance moyenne C1'-C1'\n",
    "                result[i] = (coords[prev_valid][0] + dx * (i - prev_valid),\n",
    "                           coords[prev_valid][1],\n",
    "                           coords[prev_valid][2])\n",
    "            elif next_valid is not None:\n",
    "                # Extrapolation avant le premier point valide\n",
    "                dx = 5.9\n",
    "                result[i] = (coords[next_valid][0] - dx * (next_valid - i),\n",
    "                           coords[next_valid][1],\n",
    "                           coords[next_valid][2])\n",
    "\n",
    "    return result\n",
    "\n",
    "# Test de pr\u00e9diction\n",
    "print(\"\ud83e\uddec Test de pr\u00e9diction de structure...\")\n",
    "if best_templates:\n",
    "    pred_coords = predict_structure_from_template(test_seq, best_templates[0])\n",
    "    valid_coords = sum(1 for c in pred_coords if c != (0.0, 0.0, 0.0))\n",
    "    print(f\"   Coordonn\u00e9es pr\u00e9dites: {len(pred_coords)} r\u00e9sidus, {valid_coords} valides\")\n",
    "    print(f\"   Premiers r\u00e9sidus: {pred_coords[:3]}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.5 G\u00e9n\u00e9ration de 5 pr\u00e9dictions diversifi\u00e9es\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_diverse_predictions(query_seq, templates_db, n_predictions=5):\n",
    "    \"\"\"\n",
    "    G\u00e9n\u00e8re 5 pr\u00e9dictions diversifi\u00e9es pour une s\u00e9quence.\n",
    "\n",
    "    Strat\u00e9gies de diversification :\n",
    "    1. Utiliser les top-5 templates diff\u00e9rents\n",
    "    2. Ajouter du bruit aux coordonn\u00e9es\n",
    "    3. Combiner plusieurs templates\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    # Trouver les meilleurs templates\n",
    "    best_templates = find_best_templates(query_seq, templates_db, top_k=10)\n",
    "\n",
    "    if not best_templates:\n",
    "        # Fallback : pr\u00e9diction lin\u00e9aire\n",
    "        for i in range(n_predictions):\n",
    "            coords = [(j * 5.9 + np.random.normal(0, 0.5),\n",
    "                      np.random.normal(0, 1),\n",
    "                      np.random.normal(0, 1)) for j in range(len(query_seq))]\n",
    "            predictions.append(coords)\n",
    "        return predictions\n",
    "\n",
    "    # Strat\u00e9gie 1-3 : Utiliser les 3 meilleurs templates\n",
    "    for i in range(min(3, len(best_templates))):\n",
    "        noise = i * 0.2  # Augmenter le bruit progressivement\n",
    "        coords = predict_structure_from_template(query_seq, best_templates[i], noise_std=noise)\n",
    "        predictions.append(coords)\n",
    "\n",
    "    # Strat\u00e9gie 4 : Moyenne des 2 meilleurs templates + bruit\n",
    "    if len(best_templates) >= 2:\n",
    "        coords1 = predict_structure_from_template(query_seq, best_templates[0])\n",
    "        coords2 = predict_structure_from_template(query_seq, best_templates[1])\n",
    "\n",
    "        avg_coords = []\n",
    "        for c1, c2 in zip(coords1, coords2):\n",
    "            avg = ((c1[0] + c2[0]) / 2 + np.random.normal(0, 0.3),\n",
    "                   (c1[1] + c2[1]) / 2 + np.random.normal(0, 0.3),\n",
    "                   (c1[2] + c2[2]) / 2 + np.random.normal(0, 0.3))\n",
    "            avg_coords.append(avg)\n",
    "        predictions.append(avg_coords)\n",
    "    else:\n",
    "        predictions.append(predict_structure_from_template(query_seq, best_templates[0], noise_std=0.5))\n",
    "\n",
    "    # Strat\u00e9gie 5 : Meilleur template avec perturbation al\u00e9atoire\n",
    "    coords = predict_structure_from_template(query_seq, best_templates[0], noise_std=0.8)\n",
    "    predictions.append(coords)\n",
    "\n",
    "    # S'assurer qu'on a exactement 5 pr\u00e9dictions\n",
    "    while len(predictions) < n_predictions:\n",
    "        predictions.append(predict_structure_from_template(query_seq, best_templates[0],\n",
    "                                                          noise_std=np.random.uniform(0.3, 1.0)))\n",
    "\n",
    "    return predictions[:n_predictions]\n",
    "\n",
    "# Test\n",
    "print(\"\ud83c\udfaf G\u00e9n\u00e9ration de 5 pr\u00e9dictions diversifi\u00e9es...\")\n",
    "test_predictions = generate_diverse_predictions(test_seq, templates_db, n_predictions=5)\n",
    "print(f\"   Nombre de pr\u00e9dictions: {len(test_predictions)}\")\n",
    "for i, pred in enumerate(test_predictions):\n",
    "    print(f\"   Pr\u00e9diction {i+1}: {len(pred)} r\u00e9sidus\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83d\udcca 11. \u00c9valuation locale (TM-score simplifi\u00e9)\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_tm_score_simple(pred_coords, ref_coords):\n",
    "    \"\"\"\n",
    "    Calcule un TM-score simplifi\u00e9 entre les coordonn\u00e9es pr\u00e9dites et de r\u00e9f\u00e9rence.\n",
    "    Note: Version simplifi\u00e9e sans l'optimisation de rotation/translation.\n",
    "    \"\"\"\n",
    "    n = len(ref_coords)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculer d0\n",
    "    if n >= 30:\n",
    "        d0 = 0.6 * (n - 0.5) ** (1/3) - 2.5\n",
    "    else:\n",
    "        d0_values = {12: 0.3, 15: 0.4, 19: 0.5, 23: 0.6, 29: 0.7}\n",
    "        d0 = 0.3\n",
    "        for threshold, value in d0_values.items():\n",
    "            if n >= threshold:\n",
    "                d0 = value\n",
    "\n",
    "    d0 = max(d0, 0.5)  # Minimum d0\n",
    "\n",
    "    # Centrer les coordonn\u00e9es\n",
    "    pred_arr = np.array(pred_coords)\n",
    "    ref_arr = np.array(ref_coords)\n",
    "\n",
    "    pred_centered = pred_arr - np.mean(pred_arr, axis=0)\n",
    "    ref_centered = ref_arr - np.mean(ref_arr, axis=0)\n",
    "\n",
    "    # Calculer les distances\n",
    "    distances = np.sqrt(np.sum((pred_centered - ref_centered) ** 2, axis=1))\n",
    "\n",
    "    # Calculer le TM-score\n",
    "    tm_score = np.sum(1 / (1 + (distances / d0) ** 2)) / n\n",
    "\n",
    "    return tm_score\n",
    "\n",
    "# \u00c9valuation sur le jeu de validation\n",
    "print(\"\ud83d\udcca \u00c9valuation sur le jeu de validation...\")\n",
    "\n",
    "# Prendre quelques exemples de validation\n",
    "n_eval = min(10, len(validation_sequences))\n",
    "eval_scores = []\n",
    "\n",
    "for idx in range(n_eval):\n",
    "    row = validation_sequences.iloc[idx]\n",
    "    target_id = row['target_id']\n",
    "    query_seq = row['sequence']\n",
    "\n",
    "    # Obtenir les coordonn\u00e9es de r\u00e9f\u00e9rence\n",
    "    ref_data = validation_labels[validation_labels['ID'].str.startswith(target_id + '_')]\n",
    "    if len(ref_data) == 0:\n",
    "        continue\n",
    "\n",
    "    ref_data = ref_data.sort_values('resid')\n",
    "    ref_coords = list(zip(ref_data['x_1'].fillna(0),\n",
    "                          ref_data['y_1'].fillna(0),\n",
    "                          ref_data['z_1'].fillna(0)))\n",
    "\n",
    "    # G\u00e9n\u00e9rer les pr\u00e9dictions\n",
    "    predictions = generate_diverse_predictions(query_seq, templates_db)\n",
    "\n",
    "    # Calculer le meilleur TM-score (best of 5)\n",
    "    best_score = 0\n",
    "    for pred in predictions:\n",
    "        # Ajuster la longueur si n\u00e9cessaire\n",
    "        if len(pred) != len(ref_coords):\n",
    "            min_len = min(len(pred), len(ref_coords))\n",
    "            pred = pred[:min_len]\n",
    "            ref = ref_coords[:min_len]\n",
    "        else:\n",
    "            ref = ref_coords\n",
    "\n",
    "        score = calculate_tm_score_simple(pred, ref)\n",
    "        best_score = max(best_score, score)\n",
    "\n",
    "    eval_scores.append({'target_id': target_id, 'tm_score': best_score, 'length': len(query_seq)})\n",
    "    print(f\"   {target_id}: TM-score = {best_score:.4f} (len={len(query_seq)})\")\n",
    "\n",
    "if eval_scores:\n",
    "    mean_score = np.mean([s['tm_score'] for s in eval_scores])\n",
    "    print(f\"\\n\ud83d\udcc8 TM-score moyen (validation): {mean_score:.4f}\")\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83d\udcdd 12. G\u00e9n\u00e9ration du fichier de soumission\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_submission(test_sequences_df, templates_db, output_path='submission.csv'):\n",
    "    \"\"\"\n",
    "    G\u00e9n\u00e8re le fichier de soumission au format Kaggle.\n",
    "    \"\"\"\n",
    "    print(\"\ud83d\udcdd G\u00e9n\u00e9ration du fichier de soumission...\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for idx, row in test_sequences_df.iterrows():\n",
    "        target_id = row['target_id']\n",
    "        query_seq = row['sequence']\n",
    "\n",
    "        # G\u00e9n\u00e9rer 5 pr\u00e9dictions\n",
    "        predictions = generate_diverse_predictions(query_seq, templates_db)\n",
    "\n",
    "        # Cr\u00e9er les lignes pour chaque r\u00e9sidu\n",
    "        for resid, nucleotide in enumerate(query_seq, start=1):\n",
    "            row_data = {\n",
    "                'ID': f\"{target_id}_{resid}\",\n",
    "                'resname': nucleotide,\n",
    "                'resid': resid\n",
    "            }\n",
    "\n",
    "            # Ajouter les coordonn\u00e9es pour chaque pr\u00e9diction\n",
    "            for pred_idx, pred_coords in enumerate(predictions, start=1):\n",
    "                if resid - 1 < len(pred_coords):\n",
    "                    x, y, z = pred_coords[resid - 1]\n",
    "                else:\n",
    "                    x, y, z = 0.0, 0.0, 0.0\n",
    "\n",
    "                # Clipper les coordonn\u00e9es selon les r\u00e8gles\n",
    "                x = np.clip(x, -999.999, 9999.999)\n",
    "                y = np.clip(y, -999.999, 9999.999)\n",
    "                z = np.clip(z, -999.999, 9999.999)\n",
    "\n",
    "                row_data[f'x_{pred_idx}'] = round(x, 3)\n",
    "                row_data[f'y_{pred_idx}'] = round(y, 3)\n",
    "                row_data[f'z_{pred_idx}'] = round(z, 3)\n",
    "\n",
    "            rows.append(row_data)\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"   Progression: {idx + 1}/{len(test_sequences_df)}\")\n",
    "\n",
    "    # Cr\u00e9er le DataFrame\n",
    "    submission_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Ordonner les colonnes\n",
    "    coord_cols = []\n",
    "    for i in range(1, 6):\n",
    "        coord_cols.extend([f'x_{i}', f'y_{i}', f'z_{i}'])\n",
    "\n",
    "    column_order = ['ID', 'resname', 'resid'] + coord_cols\n",
    "    submission_df = submission_df[column_order]\n",
    "\n",
    "    # Sauvegarder\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"\u2705 Soumission g\u00e9n\u00e9r\u00e9e: {output_path}\")\n",
    "    print(f\"   Shape: {submission_df.shape}\")\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "# G\u00e9n\u00e9ration de la soumission (d\u00e9commentez pour ex\u00e9cuter)\n",
    "# submission = generate_submission(test_sequences, templates_db, 'submission.csv')\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83c\udfaf 13. R\u00e9sum\u00e9 de la strat\u00e9gie\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"               \ud83c\udfaf R\u00c9SUM\u00c9 DE LA STRAT\u00c9GIE TBM\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "\ud83d\udccb PIPELINE IMPL\u00c9MENT\u00c9:\n",
    "\n",
    "1. EXTRACTION DES TEMPLATES\n",
    "   \u2022 Parser les fichiers CIF du PDB_RNA\n",
    "   \u2022 Extraire s\u00e9quences + coordonn\u00e9es C1'\n",
    "   \u2022 {n_templates} templates disponibles\n",
    "\n",
    "2. RECHERCHE DE TEMPLATES\n",
    "   \u2022 Alignement de s\u00e9quence simple\n",
    "   \u2022 Score combin\u00e9: similarit\u00e9 + ratio de longueur\n",
    "   \u2022 S\u00e9lection des top-K templates\n",
    "\n",
    "3. PR\u00c9DICTION DE STRUCTURE\n",
    "   \u2022 Copie des coordonn\u00e9es du template\n",
    "   \u2022 Interpolation des positions manquantes\n",
    "   \u2022 5 strat\u00e9gies de diversification\n",
    "\n",
    "4. DIVERSIFICATION (5 pr\u00e9dictions)\n",
    "   \u2022 Pr\u00e9diction 1-3: Top 3 templates\n",
    "   \u2022 Pr\u00e9diction 4: Moyenne des 2 meilleurs\n",
    "   \u2022 Pr\u00e9diction 5: Perturbation al\u00e9atoire\n",
    "\n",
    "\ud83d\udcc8 AM\u00c9LIORATIONS POSSIBLES:\n",
    "   \u2022 Alignement plus sophistiqu\u00e9 (Smith-Waterman)\n",
    "   \u2022 Rotation/translation optimale (Kabsch algorithm)\n",
    "   \u2022 Deep Learning pour les cibles sans template\n",
    "   \u2022 Utilisation des MSA pour l'alignement\n",
    "\n",
    "\ud83d\ude80 Pour soumettre:\n",
    "   1. D\u00e9commenter la g\u00e9n\u00e9ration de soumission\n",
    "   2. Utiliser tous les fichiers PDB (max_files=None)\n",
    "   3. Ex\u00e9cuter le notebook complet\n",
    "\"\"\".format(n_templates=len(templates_db)))\n",
    "print(\"=\" * 70)\n",
    ""
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}